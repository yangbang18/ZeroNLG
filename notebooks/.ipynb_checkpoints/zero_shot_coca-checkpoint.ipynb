{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40dddc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ZERONLG_HOME = 'data/checkpoints'\n",
    "REPO = os.path.dirname(os.path.realpath('.'))\n",
    "os.chdir(REPO)\n",
    "\n",
    "save_path = 'output/coca_results'\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f32e013-cd42-4aa8-a430-687e8f804aec",
   "metadata": {},
   "source": [
    "# CoCa Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf30292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open_clip_torch==2.14.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (2.14.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (1.13.1+cu117)\n",
      "Requirement already satisfied: torchvision in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (0.14.1+cu117)\n",
      "Requirement already satisfied: regex in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (2023.10.3)\n",
      "Requirement already satisfied: ftfy in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (6.1.1)\n",
      "Requirement already satisfied: tqdm in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (4.66.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (0.17.3)\n",
      "Requirement already satisfied: sentencepiece in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (0.1.99)\n",
      "Requirement already satisfied: protobuf==3.20.* in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (3.20.3)\n",
      "Requirement already satisfied: timm in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (0.9.10)\n",
      "Requirement already satisfied: typing-extensions in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.14.0) (4.8.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from ftfy->open_clip_torch==2.14.0) (0.2.8)\n",
      "Requirement already satisfied: filelock in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (3.12.4)\n",
      "Requirement already satisfied: fsspec in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (23.2)\n",
      "Requirement already satisfied: safetensors in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from timm->open_clip_torch==2.14.0) (0.4.0)\n",
      "Requirement already satisfied: numpy in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from torchvision->open_clip_torch==2.14.0) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from torchvision->open_clip_torch==2.14.0) (10.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch==2.14.0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch==2.14.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch==2.14.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch==2.14.0) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install open_clip_torch==2.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81782fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "from open_clip.coca_model import CoCa as open_clip_coca\n",
    "from open_clip.coca_model import prepare_inputs_for_generation\n",
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    ")\n",
    "class CoCa(open_clip_coca):\n",
    "    def _encode_image(self, images, normalize=True):\n",
    "        is_video = False\n",
    "        B, T = images.shape[:2]\n",
    "        if images.dim() == 5:\n",
    "            if T > 1:\n",
    "                is_video = True\n",
    "                images = images.view(B * T, *images.shape[2:])\n",
    "            else:\n",
    "                images = images.squeeze(1)\n",
    "            \n",
    "        image_latent, tokens_embs = self.visual(images)\n",
    "        \n",
    "        if is_video:\n",
    "            tokens_embs = image_latent.view(B, T, -1)\n",
    "            \n",
    "        image_latent = F.normalize(image_latent, dim=-1) if normalize else image_latent\n",
    "        return image_latent, tokens_embs\n",
    "    \n",
    "    def _generate_beamsearch(\n",
    "            self,\n",
    "            image_inputs,\n",
    "            pad_token_id=None,\n",
    "            eos_token_id=None,\n",
    "            sot_token_id=None,\n",
    "            num_beams=6,\n",
    "            num_beam_groups=3,\n",
    "            min_seq_len=5,\n",
    "            stopping_criteria=None,\n",
    "            logit_processor=None,\n",
    "            logit_warper=None,\n",
    "    ):\n",
    "        device = image_inputs.device\n",
    "        batch_size = image_inputs.shape[0]\n",
    "        image_inputs = torch.repeat_interleave(image_inputs, num_beams, dim=0)\n",
    "        image_latent, image_embs = self._encode_image(image_inputs)\n",
    "\n",
    "        input_ids = torch.ones((batch_size * num_beams, 1), device=device, dtype=torch.long)\n",
    "        input_ids = input_ids * sot_token_id\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=batch_size,\n",
    "            num_beams=num_beams,\n",
    "            device=device,\n",
    "            num_beam_groups=num_beam_groups,\n",
    "        )\n",
    "        # instantiate logits processors\n",
    "        logits_processor = (\n",
    "            LogitsProcessorList([MinLengthLogitsProcessor(min_seq_len, eos_token_id=eos_token_id)])\n",
    "            if logit_processor is None\n",
    "            else logit_processor\n",
    "        )\n",
    "        \n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "        num_beam_groups = beam_scorer.num_beam_groups\n",
    "        num_sub_beams = num_beams // num_beam_groups\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "        beam_indices = None\n",
    "\n",
    "        if num_beams * batch_size != batch_beam_size:\n",
    "            raise ValueError(\n",
    "                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "            )\n",
    "\n",
    "        beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)\n",
    "        # initialise score of first beam of each group with 0 and the rest with 1e-9. This ensures that the beams in\n",
    "        # the same group don't produce same tokens everytime.\n",
    "        beam_scores[:, ::num_sub_beams] = 0\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # predicted tokens in cur_len step\n",
    "            current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)\n",
    "\n",
    "            # indices which will form the beams in the next time step\n",
    "            reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)\n",
    "\n",
    "            # do one decoder step on all beams of all sentences in batch\n",
    "            model_inputs = prepare_inputs_for_generation(input_ids=input_ids, image_inputs=image_inputs)\n",
    "            outputs = self(\n",
    "                model_inputs['images'],\n",
    "                model_inputs['text'],\n",
    "                embed_cls=False,\n",
    "                image_latent=image_latent,\n",
    "                image_embs=image_embs\n",
    "            )\n",
    "            \n",
    "            for beam_group_idx in range(num_beam_groups):\n",
    "                group_start_idx = beam_group_idx * num_sub_beams\n",
    "                group_end_idx = min(group_start_idx + num_sub_beams, num_beams)\n",
    "                group_size = group_end_idx - group_start_idx\n",
    "\n",
    "                # indices of beams of current group among all sentences in batch\n",
    "                batch_group_indices = []\n",
    "\n",
    "                for batch_idx in range(batch_size):\n",
    "                    batch_group_indices.extend(\n",
    "                        [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]\n",
    "                    )\n",
    "                group_input_ids = input_ids[batch_group_indices]\n",
    "\n",
    "                # select outputs of beams of currentg group only\n",
    "                next_token_logits = outputs['logits'][batch_group_indices, -1, :]\n",
    "                vocab_size = next_token_logits.shape[-1]\n",
    "\n",
    "                next_token_scores_processed = logits_processor(\n",
    "                    group_input_ids, next_token_logits, current_tokens=current_tokens, beam_group_idx=beam_group_idx\n",
    "                )\n",
    "                next_token_scores = next_token_scores_processed + beam_scores[batch_group_indices].unsqueeze(-1)\n",
    "                next_token_scores = next_token_scores.expand_as(next_token_scores_processed)\n",
    "\n",
    "                # reshape for beam search\n",
    "                next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)\n",
    "\n",
    "                next_token_scores, next_tokens = torch.topk(\n",
    "                    next_token_scores, 2 * group_size, dim=1, largest=True, sorted=True\n",
    "                )\n",
    "\n",
    "                next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "                next_tokens = next_tokens % vocab_size\n",
    "\n",
    "                # stateless\n",
    "                process_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n",
    "                beam_outputs = beam_scorer.process(\n",
    "                    group_input_ids,\n",
    "                    next_token_scores,\n",
    "                    next_tokens,\n",
    "                    next_indices,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    eos_token_id=eos_token_id,\n",
    "                )\n",
    "                beam_scores[batch_group_indices] = beam_outputs[\"next_beam_scores\"]\n",
    "                beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "                beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "                input_ids[batch_group_indices] = group_input_ids[beam_idx]\n",
    "                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "                current_tokens[batch_group_indices] = group_input_ids[:, -1]\n",
    "\n",
    "                # (beam_idx // group_size) -> batch_idx\n",
    "                # (beam_idx % group_size) -> offset of idx inside the group\n",
    "                reordering_indices[batch_group_indices] = (\n",
    "                    num_beams * torch.div(beam_idx, group_size, rounding_mode=\"floor\") + group_start_idx + (beam_idx % group_size)\n",
    "                )\n",
    "\n",
    "            input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            # increase cur_len\n",
    "            cur_len = cur_len + 1\n",
    "            if beam_scorer.is_done or stopping_criteria(input_ids, None):\n",
    "                break\n",
    "\n",
    "        final_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids,\n",
    "            beam_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            max_length=stopping_criteria.max_length,\n",
    "        )\n",
    "        return sequence_outputs['sequences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "933290ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from open_clip.constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\n",
    "from open_clip.model import CLIP, CustomTextCLIP, convert_weights_to_lp, convert_to_custom_text_state_dict,\\\n",
    "    resize_pos_embed, get_cast_dtype\n",
    "from open_clip.loss import ClipLoss, DistillClipLoss, CoCaLoss\n",
    "from open_clip.openai import load_openai_model\n",
    "from open_clip.pretrained import is_pretrained_cfg, get_pretrained_cfg, download_pretrained, list_pretrained_tags_by_model, download_pretrained_from_hf\n",
    "from open_clip.transform import image_transform, AugmentationCfg\n",
    "from open_clip.tokenizer import HFTokenizer, tokenize\n",
    "from open_clip.factory import HF_HUB_PREFIX, get_model_config, load_checkpoint\n",
    "\n",
    "\n",
    "def create_model(\n",
    "        model_name: str,\n",
    "        pretrained: Optional[str] = None,\n",
    "        precision: str = 'fp32',\n",
    "        device: Union[str, torch.device] = 'cpu',\n",
    "        jit: bool = False,\n",
    "        force_quick_gelu: bool = False,\n",
    "        force_custom_text: bool = False,\n",
    "        force_patch_dropout: Optional[float] = None,\n",
    "        force_image_size: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "        pretrained_image: bool = False,\n",
    "        pretrained_hf: bool = True,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        output_dict: Optional[bool] = None,\n",
    "        require_pretrained: bool = False,\n",
    "):\n",
    "    has_hf_hub_prefix = model_name.startswith(HF_HUB_PREFIX)\n",
    "    if has_hf_hub_prefix:\n",
    "        model_id = model_name[len(HF_HUB_PREFIX):]\n",
    "        checkpoint_path = download_pretrained_from_hf(model_id, cache_dir=cache_dir)\n",
    "        config_path = download_pretrained_from_hf(model_id, filename='open_clip_config.json', cache_dir=cache_dir)\n",
    "\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        pretrained_cfg = config['preprocess_cfg']\n",
    "        model_cfg = config['model_cfg']\n",
    "    else:\n",
    "        model_name = model_name.replace('/', '-')  # for callers using old naming with / in ViT names\n",
    "        checkpoint_path = None\n",
    "        pretrained_cfg = {}\n",
    "        model_cfg = None\n",
    "\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    if pretrained and pretrained.lower() == 'openai':\n",
    "        logging.info(f'Loading pretrained {model_name} from OpenAI.')\n",
    "        model = load_openai_model(\n",
    "            model_name,\n",
    "            precision=precision,\n",
    "            device=device,\n",
    "            jit=jit,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "\n",
    "        # to always output dict even if it is clip\n",
    "        if output_dict and hasattr(model, \"output_dict\"):\n",
    "            model.output_dict = True\n",
    "    else:\n",
    "        model_cfg = model_cfg or get_model_config(model_name)\n",
    "        if model_cfg is not None:\n",
    "            logging.info(f'Loaded {model_name} model config.')\n",
    "        else:\n",
    "            logging.error(f'Model config for {model_name} not found; available models {list_models()}.')\n",
    "            raise RuntimeError(f'Model config for {model_name} not found.')\n",
    "\n",
    "        if force_quick_gelu:\n",
    "            # override for use of QuickGELU on non-OpenAI transformer models\n",
    "            model_cfg[\"quick_gelu\"] = True\n",
    "\n",
    "        if force_patch_dropout is not None:\n",
    "            # override the default patch dropout value\n",
    "            model_cfg[\"vision_cfg\"][\"patch_dropout\"] = force_patch_dropout\n",
    "\n",
    "        if force_image_size is not None:\n",
    "            # override model config's image size\n",
    "            model_cfg[\"vision_cfg\"][\"image_size\"] = force_image_size\n",
    "\n",
    "        if pretrained_image:\n",
    "            if 'timm_model_name' in model_cfg.get('vision_cfg', {}):\n",
    "                # pretrained weight loading for timm models set via vision_cfg\n",
    "                model_cfg['vision_cfg']['timm_model_pretrained'] = True\n",
    "            else:\n",
    "                assert False, 'pretrained image towers currently only supported for timm models'\n",
    "\n",
    "        cast_dtype = get_cast_dtype(precision)\n",
    "        is_hf_model = 'hf_model_name' in model_cfg.get('text_cfg', {})\n",
    "        custom_text = model_cfg.pop('custom_text', False) or force_custom_text or is_hf_model\n",
    "\n",
    "        if custom_text:\n",
    "            if is_hf_model:\n",
    "                model_cfg['text_cfg']['hf_model_pretrained'] = pretrained_hf\n",
    "            if \"coca\" in model_name:\n",
    "                model = CoCa(**model_cfg, cast_dtype=cast_dtype)\n",
    "            else:\n",
    "                model = CustomTextCLIP(**model_cfg, cast_dtype=cast_dtype)\n",
    "        else:\n",
    "            model = CLIP(**model_cfg, cast_dtype=cast_dtype)\n",
    "\n",
    "        pretrained_loaded = False\n",
    "        if pretrained:\n",
    "            checkpoint_path = ''\n",
    "            pretrained_cfg = get_pretrained_cfg(model_name, pretrained)\n",
    "            if pretrained_cfg:\n",
    "                checkpoint_path = download_pretrained(pretrained_cfg, cache_dir=cache_dir)\n",
    "            elif os.path.exists(pretrained):\n",
    "                checkpoint_path = pretrained\n",
    "\n",
    "            if checkpoint_path:\n",
    "                logging.info(f'Loading pretrained {model_name} weights ({pretrained}).')\n",
    "                load_checkpoint(model, checkpoint_path)\n",
    "            else:\n",
    "                error_str = (\n",
    "                    f'Pretrained weights ({pretrained}) not found for model {model_name}.'\n",
    "                    f'Available pretrained tags ({list_pretrained_tags_by_model(model_name)}.')\n",
    "                logging.warning(error_str)\n",
    "                raise RuntimeError(error_str)\n",
    "            pretrained_loaded = True\n",
    "        elif has_hf_hub_prefix:\n",
    "            logging.info(f'Loading pretrained {model_name} weights ({pretrained}).')\n",
    "            load_checkpoint(model, checkpoint_path)\n",
    "            pretrained_loaded = True\n",
    "\n",
    "        if require_pretrained and not pretrained_loaded:\n",
    "            # callers of create_model_from_pretrained always expect pretrained weights\n",
    "            raise RuntimeError(\n",
    "                f'Pretrained weights were required for (model: {model_name}, pretrained: {pretrained}) but not loaded.')\n",
    "\n",
    "        model.to(device=device)\n",
    "        if precision in (\"fp16\", \"bf16\"):\n",
    "            convert_weights_to_lp(model, dtype=torch.bfloat16 if precision == 'bf16' else torch.float16)\n",
    "\n",
    "        # set image / mean metadata from pretrained_cfg if available, or use default\n",
    "        model.visual.image_mean = pretrained_cfg.get('mean', None) or OPENAI_DATASET_MEAN\n",
    "        model.visual.image_std = pretrained_cfg.get('std', None) or OPENAI_DATASET_STD\n",
    "\n",
    "        # to always output dict even if it is clip\n",
    "        if output_dict and hasattr(model, \"output_dict\"):\n",
    "            model.output_dict = True\n",
    "\n",
    "        if jit:\n",
    "            model = torch.jit.script(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_and_transforms(\n",
    "        model_name: str,\n",
    "        pretrained: Optional[str] = None,\n",
    "        precision: str = 'fp32',\n",
    "        device: Union[str, torch.device] = 'cpu',\n",
    "        jit: bool = False,\n",
    "        force_quick_gelu: bool = False,\n",
    "        force_custom_text: bool = False,\n",
    "        force_patch_dropout: Optional[float] = None,\n",
    "        force_image_size: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "        pretrained_image: bool = False,\n",
    "        pretrained_hf: bool = True,\n",
    "        image_mean: Optional[Tuple[float, ...]] = None,\n",
    "        image_std: Optional[Tuple[float, ...]] = None,\n",
    "        aug_cfg: Optional[Union[Dict[str, Any], AugmentationCfg]] = None,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        output_dict: Optional[bool] = None,\n",
    "):\n",
    "    model = create_model(\n",
    "        model_name,\n",
    "        pretrained,\n",
    "        precision=precision,\n",
    "        device=device,\n",
    "        jit=jit,\n",
    "        force_quick_gelu=force_quick_gelu,\n",
    "        force_custom_text=force_custom_text,\n",
    "        force_patch_dropout=force_patch_dropout,\n",
    "        force_image_size=force_image_size,\n",
    "        pretrained_image=pretrained_image,\n",
    "        pretrained_hf=pretrained_hf,\n",
    "        cache_dir=cache_dir,\n",
    "        output_dict=output_dict,\n",
    "    )\n",
    "\n",
    "    image_mean = image_mean or getattr(model.visual, 'image_mean', None)\n",
    "    image_std = image_std or getattr(model.visual, 'image_std', None)\n",
    "    preprocess_train = image_transform(\n",
    "        model.visual.image_size,\n",
    "        is_train=True,\n",
    "        mean=image_mean,\n",
    "        std=image_std,\n",
    "        aug_cfg=aug_cfg,\n",
    "    )\n",
    "    preprocess_val = image_transform(\n",
    "        model.visual.image_size,\n",
    "        is_train=False,\n",
    "        mean=image_mean,\n",
    "        std=image_std,\n",
    "    )\n",
    "\n",
    "    return model, preprocess_train, preprocess_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b93de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f45980bc550>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin\n",
      "WARNING:huggingface_hub.utils._http:'HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f45980bc550>, 'Connection to huggingface.co timed out. (connect timeout=10)'))' thrown while requesting HEAD https://huggingface.co/laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model, _, transform = create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-B-32\",\n",
    "  pretrained=\"laion2B-s13B-b90k\",\n",
    "  device='cuda',\n",
    "  cache_dir=ZERONLG_HOME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9549cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2990/2990 [36:38<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msrvtt en [{'image_id': 7010, 'caption': 'how to download free mp 3 from youtube to your computer '}, {'image_id': 7011, 'caption': 'pink color in english '}, {'image_id': 7012, 'caption': 'watch this video of ed sheeran on the today show '}, {'image_id': 7013, 'caption': 'how to use the fv - 1 2 1 1 to decode a set of 2 2 '}, {'image_id': 7014, 'caption': 'the x - factor 2 0 1 5 : who is the new face of the show ? '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [1:07:22<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco en [{'image_id': 391895, 'caption': 'a man on a mountain bike in the huanglong national park , huanglong , china '}, {'image_id': 60623, 'caption': 'a group of people are eating a big bowl of food . one is a woman is one of '}, {'image_id': 483108, 'caption': 'a man is waiting for a train at the train station in the city of luang prabang '}, {'image_id': 384213, 'caption': 'the kitchen is one of the more well - known in the world . '}, {'image_id': 386164, 'caption': 'photo of a collection of wooden kitchen tools '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [21:01<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vatex zh [{'image_id': 27439, 'caption': 'watch this video of a man who is a new day after a 2 1 - year - old '}, {'image_id': 27440, 'caption': 'watch this video of a man who is a black man who is a white man who is a '}, {'image_id': 27441, 'caption': 'watch this baby play with a bucket of water '}, {'image_id': 27442, 'caption': 'a white paint roller is used to paint a wooden wall in a house . - paint roller stock '}, {'image_id': 27443, 'caption': 'how to clean a white tiled floor with a mops '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:43<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickr30k zh [{'image_id': 1009692167, 'caption': 'a new group of new canaan police department k - 9 s is set to be a part '}, {'image_id': 1021439420, 'caption': 'photo : the family is all set up for a day of family fun . this is one of '}, {'image_id': 1032122270, 'caption': 'two english cocker spaniels and a great dane in a field '}, {'image_id': 1043819504, 'caption': 'photo : the first day of the 2 0 1 2 ice age day camp . '}, {'image_id': 1095580424, 'caption': 'a black and white dog is running in a large bag of food . '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:48<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickr30k de [{'image_id': 1007129816, 'caption': 'a man with a hat made out of a 1 2 - pack of 1 2 - pack of '}, {'image_id': 1009434119, 'caption': 'english : a boston is a small english dog . '}, {'image_id': 101362133, 'caption': '2 0 1 2 world cup of karate - day 1 '}, {'image_id': 102617084, 'caption': 'a group of people in a snow day . one is a white man in a red jacket and '}, {'image_id': 10287332, 'caption': 'new home construction - new home construction is a great time to do home projects . we can help '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:43<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickr30k fr [{'image_id': 1007129816, 'caption': 'a man with a hat made out of a 1 2 - pack of 1 2 - pack of '}, {'image_id': 1009434119, 'caption': 'english : a boston is a small english dog . '}, {'image_id': 101362133, 'caption': '2 0 1 2 world cup of karate - day 1 '}, {'image_id': 102617084, 'caption': 'a group of people in a snow day . one is a white man in a red jacket and '}, {'image_id': 10287332, 'caption': 'new home construction - new home construction is a great time to do home projects . we can help '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import configs\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from zeronlg import CaptionDataset\n",
    "\n",
    "all_datasets = [\n",
    "    'msrvtt',\n",
    "    'coco',\n",
    "    'vatex',\n",
    "    'flickr30k',\n",
    "]\n",
    "all_langs = [\n",
    "    ['en'],\n",
    "    ['en'],\n",
    "    ['zh'],\n",
    "    ['zh', 'de', 'fr'],\n",
    "]\n",
    "\n",
    "generation_kwargs = {\n",
    "    'num_beams': 3,\n",
    "    'min_seq_len': 3,\n",
    "    'seq_len': 20\n",
    "}\n",
    "\n",
    "for tag, langs in zip(all_datasets, all_langs):\n",
    "    for lang in langs:\n",
    "        dataset = CaptionDataset(\n",
    "            vision_root=configs.image_video_root[tag],\n",
    "            ann_rpath=f'{configs.annotation_root}/{tag}/{lang}/test.json',\n",
    "            lang=lang,\n",
    "            return_images=True,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=1,\n",
    "            sampler=None,\n",
    "            shuffle=False,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for batch in tqdm(loader):\n",
    "            image_ids, images = batch\n",
    "            image = images[0][len(images[0]) // 2] # get the middle frame\n",
    "            im = [transform(image).unsqueeze(0)]\n",
    "            im = torch.stack(im, dim=1).to('cuda')\n",
    "\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                generated = model.generate(im, **generation_kwargs)\n",
    "\n",
    "            caption = open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\")\n",
    "            results.append({\"image_id\": image_ids[0], \"caption\": caption})\n",
    "\n",
    "        print(tag, lang, results[:5])\n",
    "        results_file = os.path.join(save_path, f'coca-b-32_{lang}_{tag}.json')\n",
    "        json.dump(results, open(results_file, 'w'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c71775b9",
   "metadata": {},
   "source": [
    "# Translate CoCa's results with NLLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dec28f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.27.1\n",
      "  Using cached transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "Requirement already satisfied: filelock in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.1)\n",
      "  Using cached tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.27.1) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.27.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.27.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.27.1) (2023.7.22)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.12.5\n",
      "    Uninstalling transformers-4.12.5:\n",
      "      Successfully uninstalled transformers-4.12.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "zeronlg 1.0.1 requires transformers==4.12.5, but you have transformers 4.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.27.1\n"
     ]
    }
   ],
   "source": [
    "# if your `transformers` version is low, e.g., 4.12.5\n",
    "# then you should upgrade it to load the NLLB model\n",
    "!pip install transformers==4.27.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cffd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "model_name = f\"{ZERONLG_HOME}/{model_name.replace('/', '_')}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37df3821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating output/coca_results/coca-b-32_zh_vatex.json ...\n",
      "save results to output/coca_results/coca-b-32_zh_vatex_NLLB.json\n",
      "translating output/coca_results/coca-b-32_zh_flickr30k.json ...\n",
      "save results to output/coca_results/coca-b-32_zh_flickr30k_NLLB.json\n",
      "translating output/coca_results/coca-b-32_de_flickr30k.json ...\n",
      "save results to output/coca_results/coca-b-32_de_flickr30k_NLLB.json\n",
      "translating output/coca_results/coca-b-32_fr_flickr30k.json ...\n",
      "save results to output/coca_results/coca-b-32_fr_flickr30k_NLLB.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from zeronlg.utils import batch_to_device\n",
    "\n",
    "tag = 'NLLB'\n",
    "mapping = {\n",
    "    'en': 'eng_Latn',\n",
    "    'zh': 'zho_Hant',\n",
    "    'de': 'deu_Latn',\n",
    "    'fr': 'fra_Latn',\n",
    "}\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "tokenizer.src_lang = mapping['en']\n",
    "\n",
    "all_datasets = [\n",
    "    'vatex',\n",
    "    'flickr30k',\n",
    "]\n",
    "all_langs = [\n",
    "    ['zh'],\n",
    "    ['zh', 'de', 'fr'],\n",
    "]\n",
    "\n",
    "for tag, langs in zip(all_datasets, all_langs):\n",
    "    for lang in langs:\n",
    "        src_path = f\"{save_path}/coca-b-32_{lang}_{tag}.json\"\n",
    "        trg_path = f\"{save_path}/coca-b-32_{lang}_{tag}_NLLB.json\"\n",
    "\n",
    "        print(f'translating {src_path} ...')\n",
    "        data = json.load(open(src_path, 'r'))\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        num_batches = len(data) // batch_size\n",
    "        if batch_size * num_batches != len(data):\n",
    "            num_batches += 1\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start, end = i * batch_size, (i + 1) * batch_size\n",
    "            text = [line['caption'] for line in data[start:end]]\n",
    "            encoded_text = tokenizer(text, return_tensors='pt', padding=True)\n",
    "            encoded_text = batch_to_device(encoded_text, device)\n",
    "            generated_tokens = model.generate(\n",
    "                **encoded_text,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[mapping[lang]]\n",
    "            )\n",
    "            res = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            for line, caption in zip(data[start:end], res):\n",
    "                line['caption'] = caption\n",
    "                results.append(line)\n",
    "\n",
    "        print(f'save results to {trg_path}')\n",
    "        with open(trg_path, 'w') as wf:\n",
    "            json.dump(results, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0853fb48-c88a-475f-8d52-37e4e827a4f4",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d343bd9f-8d72-4aa7-bf9d-6fb6f6b7b835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.12.5 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (4.12.5)\n",
      "Requirement already satisfied: filelock in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (2.31.0)\n",
      "Requirement already satisfied: sacremoses in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (0.0.53)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.12.5) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.12.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2023.7.22)\n",
      "Requirement already satisfied: six in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n",
      "Requirement already satisfied: click in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.12.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0572dc-b903-4a6d-82b4-3d577134ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:18:18 - results_file: output/coca_results/coca-b-32_en_msrvtt.json\n",
      "2023-11-18 14:18:18 - gt_file: data/annotations/msrvtt/en/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 617049 tokens at 1151339.14 tokens per second.\n",
      "PTBTokenizer tokenized 48701 tokens at 277551.22 tokens per second.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 41143, 'reflen': 37148, 'guess': [41143, 38153, 35163, 32174], 'correct': [11513, 2500, 562, 154]}\n",
      "ratio: 1.1075428017658797\n",
      "Bleu_1: 0.280\n",
      "Bleu_2: 0.135\n",
      "Bleu_3: 0.066\n",
      "Bleu_4: 0.034\n",
      "computing METEOR score...\n",
      "METEOR: 0.102\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.215\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.056\n",
      "computing SPICE score...\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.0 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [13.196 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n",
      "SPICE evaluation took: 30.97 s\n",
      "SPICE: 0.026\n",
      "Bleu_1: 0.280\n",
      "Bleu_2: 0.135\n",
      "Bleu_3: 0.066\n",
      "Bleu_4: 0.034\n",
      "METEOR: 0.102\n",
      "ROUGE_L: 0.215\n",
      "CIDEr: 0.056\n",
      "SPICE: 0.026\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_en_msrvtt.json --dataset msrvtt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63dbafce-6538-476c-80c6-2a3d21e6f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:19:22 - results_file: output/coca_results/coca-b-32_en_coco.json\n",
      "2023-11-18 14:19:22 - gt_file: data/annotations/coco/en/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 307085 tokens at 923797.80 tokens per second.\n",
      "Nov 18, 2023 2:19:23 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: 「 (U+300C, decimal: 12300)\n",
      "PTBTokenizer tokenized 80843 tokens at 310990.74 tokens per second.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 69059, 'reflen': 58042, 'guess': [69059, 64059, 59061, 54080], 'correct': [20370, 5963, 1716, 512]}\n",
      "ratio: 1.1898108266427554\n",
      "Bleu_1: 0.295\n",
      "Bleu_2: 0.166\n",
      "Bleu_3: 0.093\n",
      "Bleu_4: 0.052\n",
      "computing METEOR score...\n",
      "METEOR: 0.112\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.242\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.167\n",
      "computing SPICE score...\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.0 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Threads( StanfordCoreNLP ) [18.734 seconds]\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n",
      "SPICE evaluation took: 29.90 s\n",
      "SPICE: 0.068\n",
      "Bleu_1: 0.295\n",
      "Bleu_2: 0.166\n",
      "Bleu_3: 0.093\n",
      "Bleu_4: 0.052\n",
      "METEOR: 0.112\n",
      "ROUGE_L: 0.242\n",
      "CIDEr: 0.167\n",
      "SPICE: 0.068\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_en_coco.json --dataset coco --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a5949d-c610-46c6-8200-af895754b89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:22:39 - results_file: output/coca_results/coca-b-32_zh_vatex_NLLB.json\n",
      "2023-11-18 14:22:39 - gt_file: data/annotations/vatex/zh/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Building prefix dict from the default dictionary ...\n",
      "2023-11-18 14:22:39 - Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2023-11-18 14:22:39 - Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.601 seconds.\n",
      "2023-11-18 14:22:40 - Loading model cost 0.601 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2023-11-18 14:22:40 - Prefix dict has been built successfully.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 13812, 'reflen': 14738, 'guess': [13812, 12316, 10821, 9329], 'correct': [2158, 158, 13, 0]}\n",
      "ratio: 0.9371692224181749\n",
      "Bleu_1: 0.146\n",
      "Bleu_2: 0.042\n",
      "Bleu_3: 0.013\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n",
      "METEOR: 0.063\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.116\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.012\n",
      "Bleu_1: 0.146\n",
      "Bleu_2: 0.042\n",
      "Bleu_3: 0.013\n",
      "Bleu_4: 0.000\n",
      "METEOR: 0.063\n",
      "ROUGE_L: 0.116\n",
      "CIDEr: 0.012\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_zh_vatex_NLLB.json --dataset vatex --lang zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46814903-1824-4f62-9cfd-1aa4f6bd2cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:22:58 - results_file: output/coca_results/coca-b-32_zh_flickr30k_NLLB.json\n",
      "2023-11-18 14:22:58 - gt_file: data/annotations/flickr30k/zh/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Building prefix dict from the default dictionary ...\n",
      "2023-11-18 14:22:58 - Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2023-11-18 14:22:58 - Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.577 seconds.\n",
      "2023-11-18 14:22:59 - Loading model cost 0.577 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2023-11-18 14:22:59 - Prefix dict has been built successfully.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 8692, 'reflen': 8467, 'guess': [8692, 7693, 6694, 5699], 'correct': [1505, 154, 20, 2]}\n",
      "ratio: 1.026573756938582\n",
      "Bleu_1: 0.173\n",
      "Bleu_2: 0.059\n",
      "Bleu_3: 0.022\n",
      "Bleu_4: 0.008\n",
      "computing METEOR score...\n",
      "METEOR: 0.068\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.132\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.026\n",
      "Bleu_1: 0.173\n",
      "Bleu_2: 0.059\n",
      "Bleu_3: 0.022\n",
      "Bleu_4: 0.008\n",
      "METEOR: 0.068\n",
      "ROUGE_L: 0.132\n",
      "CIDEr: 0.026\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_zh_flickr30k_NLLB.json --dataset flickr30k --lang zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a678d67-ad4f-43ba-bf3f-60818f8594d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:23:56 - results_file: output/coca_results/coca-b-32_de_flickr30k_NLLB.json\n",
      "2023-11-18 14:23:56 - gt_file: data/annotations/flickr30k/de/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "2023-11-18 14:23:56 - Initializing native server...\n",
      "2023-11-18 14:23:56 - java -Xmx4g -cp \"/data/yb/checkpoints/stanford-corenlp-4.5.2/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000\n",
      "2023-11-18 14:23:56 - Server shell PID: 1824936\n",
      "2023-11-18 14:23:57 - The server is available.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 12443, 'reflen': 10754, 'guess': [12443, 11443, 10443, 9447], 'correct': [2928, 728, 178, 43]}\n",
      "ratio: 1.1570578389435413\n",
      "Bleu_1: 0.235\n",
      "Bleu_2: 0.122\n",
      "Bleu_3: 0.063\n",
      "Bleu_4: 0.033\n",
      "computing METEOR score...\n",
      "METEOR: 0.100\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.205\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.105\n",
      "Bleu_1: 0.235\n",
      "Bleu_2: 0.122\n",
      "Bleu_3: 0.063\n",
      "Bleu_4: 0.033\n",
      "METEOR: 0.100\n",
      "ROUGE_L: 0.205\n",
      "CIDEr: 0.105\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_de_flickr30k_NLLB.json --dataset flickr30k --lang de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e28c3b65-6124-4ec1-9ae3-6ec7f38a4954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:24:29 - results_file: output/coca_results/coca-b-32_fr_flickr30k_NLLB.json\n",
      "2023-11-18 14:24:29 - gt_file: data/annotations/flickr30k/fr/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "2023-11-18 14:24:29 - Initializing native server...\n",
      "2023-11-18 14:24:29 - java -Xmx4g -cp \"/data/yb/checkpoints/stanford-corenlp-4.5.2/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000\n",
      "2023-11-18 14:24:29 - Server shell PID: 1825457\n",
      "2023-11-18 14:24:30 - The server is available.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 14221, 'reflen': 12839, 'guess': [14221, 13221, 12221, 11222], 'correct': [2222, 522, 139, 41]}\n",
      "ratio: 1.1076407819922807\n",
      "Bleu_1: 0.156\n",
      "Bleu_2: 0.079\n",
      "Bleu_3: 0.041\n",
      "Bleu_4: 0.023\n",
      "computing METEOR score...\n",
      "METEOR: 0.078\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.153\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.188\n",
      "Bleu_1: 0.156\n",
      "Bleu_2: 0.079\n",
      "Bleu_3: 0.041\n",
      "Bleu_4: 0.023\n",
      "METEOR: 0.078\n",
      "ROUGE_L: 0.153\n",
      "CIDEr: 0.188\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_fr_flickr30k_NLLB.json --dataset flickr30k --lang fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736aac8-ae2c-4012-afc5-1f3e80244c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
