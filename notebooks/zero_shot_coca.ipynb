{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40dddc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ZERONLG_HOME = 'data/checkpoints'\n",
    "REPO = os.path.dirname(os.path.realpath('.'))\n",
    "os.chdir(REPO)\n",
    "\n",
    "save_path = 'output/coca_results'\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f32e013-cd42-4aa8-a430-687e8f804aec",
   "metadata": {},
   "source": [
    "# CoCa Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf30292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open_clip_torch==2.14.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (2.14.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (1.13.1+cu117)\n",
      "Requirement already satisfied: torchvision in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (0.14.1+cu117)\n",
      "Requirement already satisfied: regex in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (2023.10.3)\n",
      "Requirement already satisfied: ftfy in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (6.1.1)\n",
      "Requirement already satisfied: tqdm in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (4.66.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (0.17.3)\n",
      "Requirement already satisfied: sentencepiece in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (0.1.99)\n",
      "Requirement already satisfied: protobuf==3.20.* in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (3.20.3)\n",
      "Requirement already satisfied: timm in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from open_clip_torch==2.14.0) (0.9.10)\n",
      "Requirement already satisfied: typing-extensions in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from torch>=1.9.0->open_clip_torch==2.14.0) (4.8.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from ftfy->open_clip_torch==2.14.0) (0.2.8)\n",
      "Requirement already satisfied: filelock in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (3.12.4)\n",
      "Requirement already satisfied: fsspec in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub->open_clip_torch==2.14.0) (23.2)\n",
      "Requirement already satisfied: safetensors in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from timm->open_clip_torch==2.14.0) (0.4.0)\n",
      "Requirement already satisfied: numpy in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from torchvision->open_clip_torch==2.14.0) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from torchvision->open_clip_torch==2.14.0) (10.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch==2.14.0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch==2.14.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch==2.14.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->huggingface-hub->open_clip_torch==2.14.0) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install open_clip_torch==2.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81782fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "from open_clip.coca_model import CoCa as open_clip_coca\n",
    "from open_clip.coca_model import prepare_inputs_for_generation\n",
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    ")\n",
    "class CoCa(open_clip_coca):\n",
    "    def _encode_image(self, images, normalize=True):\n",
    "        is_video = False\n",
    "        B, T = images.shape[:2]\n",
    "        if images.dim() == 5:\n",
    "            if T > 1:\n",
    "                is_video = True\n",
    "                images = images.view(B * T, *images.shape[2:])\n",
    "            else:\n",
    "                images = images.squeeze(1)\n",
    "            \n",
    "        image_latent, tokens_embs = self.visual(images)\n",
    "        \n",
    "        if is_video:\n",
    "            tokens_embs = image_latent.view(B, T, -1)\n",
    "            \n",
    "        image_latent = F.normalize(image_latent, dim=-1) if normalize else image_latent\n",
    "        return image_latent, tokens_embs\n",
    "    \n",
    "    def _generate_beamsearch(\n",
    "            self,\n",
    "            image_inputs,\n",
    "            pad_token_id=None,\n",
    "            eos_token_id=None,\n",
    "            sot_token_id=None,\n",
    "            num_beams=6,\n",
    "            num_beam_groups=3,\n",
    "            min_seq_len=5,\n",
    "            stopping_criteria=None,\n",
    "            logit_processor=None,\n",
    "            logit_warper=None,\n",
    "    ):\n",
    "        device = image_inputs.device\n",
    "        batch_size = image_inputs.shape[0]\n",
    "        image_inputs = torch.repeat_interleave(image_inputs, num_beams, dim=0)\n",
    "        image_latent, image_embs = self._encode_image(image_inputs)\n",
    "\n",
    "        input_ids = torch.ones((batch_size * num_beams, 1), device=device, dtype=torch.long)\n",
    "        input_ids = input_ids * sot_token_id\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=batch_size,\n",
    "            num_beams=num_beams,\n",
    "            device=device,\n",
    "            num_beam_groups=num_beam_groups,\n",
    "        )\n",
    "        # instantiate logits processors\n",
    "        logits_processor = (\n",
    "            LogitsProcessorList([MinLengthLogitsProcessor(min_seq_len, eos_token_id=eos_token_id)])\n",
    "            if logit_processor is None\n",
    "            else logit_processor\n",
    "        )\n",
    "        \n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        num_beams = beam_scorer.num_beams\n",
    "        num_beam_groups = beam_scorer.num_beam_groups\n",
    "        num_sub_beams = num_beams // num_beam_groups\n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "        beam_indices = None\n",
    "\n",
    "        if num_beams * batch_size != batch_beam_size:\n",
    "            raise ValueError(\n",
    "                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "            )\n",
    "\n",
    "        beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)\n",
    "        # initialise score of first beam of each group with 0 and the rest with 1e-9. This ensures that the beams in\n",
    "        # the same group don't produce same tokens everytime.\n",
    "        beam_scores[:, ::num_sub_beams] = 0\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # predicted tokens in cur_len step\n",
    "            current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)\n",
    "\n",
    "            # indices which will form the beams in the next time step\n",
    "            reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)\n",
    "\n",
    "            # do one decoder step on all beams of all sentences in batch\n",
    "            model_inputs = prepare_inputs_for_generation(input_ids=input_ids, image_inputs=image_inputs)\n",
    "            outputs = self(\n",
    "                model_inputs['images'],\n",
    "                model_inputs['text'],\n",
    "                embed_cls=False,\n",
    "                image_latent=image_latent,\n",
    "                image_embs=image_embs\n",
    "            )\n",
    "            \n",
    "            for beam_group_idx in range(num_beam_groups):\n",
    "                group_start_idx = beam_group_idx * num_sub_beams\n",
    "                group_end_idx = min(group_start_idx + num_sub_beams, num_beams)\n",
    "                group_size = group_end_idx - group_start_idx\n",
    "\n",
    "                # indices of beams of current group among all sentences in batch\n",
    "                batch_group_indices = []\n",
    "\n",
    "                for batch_idx in range(batch_size):\n",
    "                    batch_group_indices.extend(\n",
    "                        [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]\n",
    "                    )\n",
    "                group_input_ids = input_ids[batch_group_indices]\n",
    "\n",
    "                # select outputs of beams of currentg group only\n",
    "                next_token_logits = outputs['logits'][batch_group_indices, -1, :]\n",
    "                vocab_size = next_token_logits.shape[-1]\n",
    "\n",
    "                next_token_scores_processed = logits_processor(\n",
    "                    group_input_ids, next_token_logits, current_tokens=current_tokens, beam_group_idx=beam_group_idx\n",
    "                )\n",
    "                next_token_scores = next_token_scores_processed + beam_scores[batch_group_indices].unsqueeze(-1)\n",
    "                next_token_scores = next_token_scores.expand_as(next_token_scores_processed)\n",
    "\n",
    "                # reshape for beam search\n",
    "                next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)\n",
    "\n",
    "                next_token_scores, next_tokens = torch.topk(\n",
    "                    next_token_scores, 2 * group_size, dim=1, largest=True, sorted=True\n",
    "                )\n",
    "\n",
    "                next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "                next_tokens = next_tokens % vocab_size\n",
    "\n",
    "                # stateless\n",
    "                process_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n",
    "                beam_outputs = beam_scorer.process(\n",
    "                    group_input_ids,\n",
    "                    next_token_scores,\n",
    "                    next_tokens,\n",
    "                    next_indices,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    eos_token_id=eos_token_id,\n",
    "                )\n",
    "                beam_scores[batch_group_indices] = beam_outputs[\"next_beam_scores\"]\n",
    "                beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "                beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "                input_ids[batch_group_indices] = group_input_ids[beam_idx]\n",
    "                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "                current_tokens[batch_group_indices] = group_input_ids[:, -1]\n",
    "\n",
    "                # (beam_idx // group_size) -> batch_idx\n",
    "                # (beam_idx % group_size) -> offset of idx inside the group\n",
    "                reordering_indices[batch_group_indices] = (\n",
    "                    num_beams * torch.div(beam_idx, group_size, rounding_mode=\"floor\") + group_start_idx + (beam_idx % group_size)\n",
    "                )\n",
    "\n",
    "            input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            # increase cur_len\n",
    "            cur_len = cur_len + 1\n",
    "            if beam_scorer.is_done or stopping_criteria(input_ids, None):\n",
    "                break\n",
    "\n",
    "        final_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids,\n",
    "            beam_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            max_length=stopping_criteria.max_length,\n",
    "        )\n",
    "        return sequence_outputs['sequences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933290ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from open_clip.constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\n",
    "from open_clip.model import CLIP, CustomTextCLIP, convert_weights_to_lp, convert_to_custom_text_state_dict,\\\n",
    "    resize_pos_embed, get_cast_dtype\n",
    "from open_clip.loss import ClipLoss, DistillClipLoss, CoCaLoss\n",
    "from open_clip.openai import load_openai_model\n",
    "from open_clip.pretrained import is_pretrained_cfg, get_pretrained_cfg, download_pretrained, list_pretrained_tags_by_model, download_pretrained_from_hf\n",
    "from open_clip.transform import image_transform, AugmentationCfg\n",
    "from open_clip.tokenizer import HFTokenizer, tokenize\n",
    "from open_clip.factory import HF_HUB_PREFIX, get_model_config, load_checkpoint\n",
    "\n",
    "\n",
    "def create_model(\n",
    "        model_name: str,\n",
    "        pretrained: Optional[str] = None,\n",
    "        precision: str = 'fp32',\n",
    "        device: Union[str, torch.device] = 'cpu',\n",
    "        jit: bool = False,\n",
    "        force_quick_gelu: bool = False,\n",
    "        force_custom_text: bool = False,\n",
    "        force_patch_dropout: Optional[float] = None,\n",
    "        force_image_size: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "        pretrained_image: bool = False,\n",
    "        pretrained_hf: bool = True,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        output_dict: Optional[bool] = None,\n",
    "        require_pretrained: bool = False,\n",
    "):\n",
    "    has_hf_hub_prefix = model_name.startswith(HF_HUB_PREFIX)\n",
    "    if has_hf_hub_prefix:\n",
    "        model_id = model_name[len(HF_HUB_PREFIX):]\n",
    "        checkpoint_path = download_pretrained_from_hf(model_id, cache_dir=cache_dir)\n",
    "        config_path = download_pretrained_from_hf(model_id, filename='open_clip_config.json', cache_dir=cache_dir)\n",
    "\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        pretrained_cfg = config['preprocess_cfg']\n",
    "        model_cfg = config['model_cfg']\n",
    "    else:\n",
    "        model_name = model_name.replace('/', '-')  # for callers using old naming with / in ViT names\n",
    "        checkpoint_path = None\n",
    "        pretrained_cfg = {}\n",
    "        model_cfg = None\n",
    "\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    if pretrained and pretrained.lower() == 'openai':\n",
    "        logging.info(f'Loading pretrained {model_name} from OpenAI.')\n",
    "        model = load_openai_model(\n",
    "            model_name,\n",
    "            precision=precision,\n",
    "            device=device,\n",
    "            jit=jit,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "\n",
    "        # to always output dict even if it is clip\n",
    "        if output_dict and hasattr(model, \"output_dict\"):\n",
    "            model.output_dict = True\n",
    "    else:\n",
    "        model_cfg = model_cfg or get_model_config(model_name)\n",
    "        if model_cfg is not None:\n",
    "            logging.info(f'Loaded {model_name} model config.')\n",
    "        else:\n",
    "            logging.error(f'Model config for {model_name} not found; available models {list_models()}.')\n",
    "            raise RuntimeError(f'Model config for {model_name} not found.')\n",
    "\n",
    "        if force_quick_gelu:\n",
    "            # override for use of QuickGELU on non-OpenAI transformer models\n",
    "            model_cfg[\"quick_gelu\"] = True\n",
    "\n",
    "        if force_patch_dropout is not None:\n",
    "            # override the default patch dropout value\n",
    "            model_cfg[\"vision_cfg\"][\"patch_dropout\"] = force_patch_dropout\n",
    "\n",
    "        if force_image_size is not None:\n",
    "            # override model config's image size\n",
    "            model_cfg[\"vision_cfg\"][\"image_size\"] = force_image_size\n",
    "\n",
    "        if pretrained_image:\n",
    "            if 'timm_model_name' in model_cfg.get('vision_cfg', {}):\n",
    "                # pretrained weight loading for timm models set via vision_cfg\n",
    "                model_cfg['vision_cfg']['timm_model_pretrained'] = True\n",
    "            else:\n",
    "                assert False, 'pretrained image towers currently only supported for timm models'\n",
    "\n",
    "        cast_dtype = get_cast_dtype(precision)\n",
    "        is_hf_model = 'hf_model_name' in model_cfg.get('text_cfg', {})\n",
    "        custom_text = model_cfg.pop('custom_text', False) or force_custom_text or is_hf_model\n",
    "\n",
    "        if custom_text:\n",
    "            if is_hf_model:\n",
    "                model_cfg['text_cfg']['hf_model_pretrained'] = pretrained_hf\n",
    "            if \"coca\" in model_name:\n",
    "                model = CoCa(**model_cfg, cast_dtype=cast_dtype)\n",
    "            else:\n",
    "                model = CustomTextCLIP(**model_cfg, cast_dtype=cast_dtype)\n",
    "        else:\n",
    "            model = CLIP(**model_cfg, cast_dtype=cast_dtype)\n",
    "\n",
    "        pretrained_loaded = False\n",
    "        if pretrained:\n",
    "            checkpoint_path = ''\n",
    "            pretrained_cfg = get_pretrained_cfg(model_name, pretrained)\n",
    "            if pretrained_cfg:\n",
    "                checkpoint_path = download_pretrained(pretrained_cfg, cache_dir=cache_dir)\n",
    "            elif os.path.exists(pretrained):\n",
    "                checkpoint_path = pretrained\n",
    "\n",
    "            if checkpoint_path:\n",
    "                logging.info(f'Loading pretrained {model_name} weights ({pretrained}).')\n",
    "                load_checkpoint(model, checkpoint_path)\n",
    "            else:\n",
    "                error_str = (\n",
    "                    f'Pretrained weights ({pretrained}) not found for model {model_name}.'\n",
    "                    f'Available pretrained tags ({list_pretrained_tags_by_model(model_name)}.')\n",
    "                logging.warning(error_str)\n",
    "                raise RuntimeError(error_str)\n",
    "            pretrained_loaded = True\n",
    "        elif has_hf_hub_prefix:\n",
    "            logging.info(f'Loading pretrained {model_name} weights ({pretrained}).')\n",
    "            load_checkpoint(model, checkpoint_path)\n",
    "            pretrained_loaded = True\n",
    "\n",
    "        if require_pretrained and not pretrained_loaded:\n",
    "            # callers of create_model_from_pretrained always expect pretrained weights\n",
    "            raise RuntimeError(\n",
    "                f'Pretrained weights were required for (model: {model_name}, pretrained: {pretrained}) but not loaded.')\n",
    "\n",
    "        model.to(device=device)\n",
    "        if precision in (\"fp16\", \"bf16\"):\n",
    "            convert_weights_to_lp(model, dtype=torch.bfloat16 if precision == 'bf16' else torch.float16)\n",
    "\n",
    "        # set image / mean metadata from pretrained_cfg if available, or use default\n",
    "        model.visual.image_mean = pretrained_cfg.get('mean', None) or OPENAI_DATASET_MEAN\n",
    "        model.visual.image_std = pretrained_cfg.get('std', None) or OPENAI_DATASET_STD\n",
    "\n",
    "        # to always output dict even if it is clip\n",
    "        if output_dict and hasattr(model, \"output_dict\"):\n",
    "            model.output_dict = True\n",
    "\n",
    "        if jit:\n",
    "            model = torch.jit.script(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_and_transforms(\n",
    "        model_name: str,\n",
    "        pretrained: Optional[str] = None,\n",
    "        precision: str = 'fp32',\n",
    "        device: Union[str, torch.device] = 'cpu',\n",
    "        jit: bool = False,\n",
    "        force_quick_gelu: bool = False,\n",
    "        force_custom_text: bool = False,\n",
    "        force_patch_dropout: Optional[float] = None,\n",
    "        force_image_size: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "        pretrained_image: bool = False,\n",
    "        pretrained_hf: bool = True,\n",
    "        image_mean: Optional[Tuple[float, ...]] = None,\n",
    "        image_std: Optional[Tuple[float, ...]] = None,\n",
    "        aug_cfg: Optional[Union[Dict[str, Any], AugmentationCfg]] = None,\n",
    "        cache_dir: Optional[str] = None,\n",
    "        output_dict: Optional[bool] = None,\n",
    "):\n",
    "    model = create_model(\n",
    "        model_name,\n",
    "        pretrained,\n",
    "        precision=precision,\n",
    "        device=device,\n",
    "        jit=jit,\n",
    "        force_quick_gelu=force_quick_gelu,\n",
    "        force_custom_text=force_custom_text,\n",
    "        force_patch_dropout=force_patch_dropout,\n",
    "        force_image_size=force_image_size,\n",
    "        pretrained_image=pretrained_image,\n",
    "        pretrained_hf=pretrained_hf,\n",
    "        cache_dir=cache_dir,\n",
    "        output_dict=output_dict,\n",
    "    )\n",
    "\n",
    "    image_mean = image_mean or getattr(model.visual, 'image_mean', None)\n",
    "    image_std = image_std or getattr(model.visual, 'image_std', None)\n",
    "    preprocess_train = image_transform(\n",
    "        model.visual.image_size,\n",
    "        is_train=True,\n",
    "        mean=image_mean,\n",
    "        std=image_std,\n",
    "        aug_cfg=aug_cfg,\n",
    "    )\n",
    "    preprocess_val = image_transform(\n",
    "        model.visual.image_size,\n",
    "        is_train=False,\n",
    "        mean=image_mean,\n",
    "        std=image_std,\n",
    "    )\n",
    "\n",
    "    return model, preprocess_train, preprocess_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b93de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f24fc4c80d0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 12615bbb-d178-4c70-b3d6-6a8eef6ef55f)')' thrown while requesting HEAD https://huggingface.co/laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin\n",
      "WARNING:huggingface_hub.utils._http:'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f24fc4c80d0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 12615bbb-d178-4c70-b3d6-6a8eef6ef55f)')' thrown while requesting HEAD https://huggingface.co/laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin\n"
     ]
    },
    {
     "ename": "LocalEntryNotFoundError",
     "evalue": "An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mtimeout\u001b[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/connectionpool.py:492\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    491\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/connectionpool.py:1097\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1097\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/connection.py:212\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x7f24fc4c80d0>, 'Connection to huggingface.co timed out. (connect timeout=10)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f24fc4c80d0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/file_download.py:1599\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1599\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1608\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/file_download.py:417\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 417\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mProxyError\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:274\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nb_tries \u001b[38;5;241m>\u001b[39m max_retries:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Sleep for X seconds\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:258\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/requests/adapters.py:507\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 507\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /laion/CoCa-ViT-B-32-laion2B-s13B-b90k/resolve/main/open_clip_pytorch_model.bin (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f24fc4c80d0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 12615bbb-d178-4c70-b3d6-6a8eef6ef55f)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, _, transform \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoca_ViT-B-32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlaion2B-s13B-b90k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mZERONLG_HOME\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 177\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, pretrained_image, pretrained_hf, image_mean, image_std, aug_cfg, cache_dir, output_dict)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_and_transforms\u001b[39m(\n\u001b[1;32m    160\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    161\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m         output_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    176\u001b[0m ):\n\u001b[0;32m--> 177\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_hf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     image_mean \u001b[38;5;241m=\u001b[39m image_mean \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model\u001b[38;5;241m.\u001b[39mvisual, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    194\u001b[0m     image_std \u001b[38;5;241m=\u001b[39m image_std \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model\u001b[38;5;241m.\u001b[39mvisual, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_std\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 118\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained)\u001b[0m\n\u001b[1;32m    116\u001b[0m pretrained_cfg \u001b[38;5;241m=\u001b[39m get_pretrained_cfg(model_name, pretrained)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained_cfg:\n\u001b[0;32m--> 118\u001b[0m     checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pretrained):\n\u001b[1;32m    120\u001b[0m     checkpoint_path \u001b[38;5;241m=\u001b[39m pretrained\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/open_clip/pretrained.py:366\u001b[0m, in \u001b[0;36mdownload_pretrained\u001b[0;34m(cfg, force_hf_hub, cache_dir)\u001b[0m\n\u001b[1;32m    364\u001b[0m         target \u001b[38;5;241m=\u001b[39m download_pretrained_from_hf(model_id, filename\u001b[38;5;241m=\u001b[39mfilename, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m         target \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_pretrained_from_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/open_clip/pretrained.py:336\u001b[0m, in \u001b[0;36mdownload_pretrained_from_hf\u001b[0;34m(model_id, filename, revision, cache_dir)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_pretrained_from_hf\u001b[39m(\n\u001b[1;32m    330\u001b[0m         model_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    331\u001b[0m         filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen_clip_pytorch_model.bin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    332\u001b[0m         revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m         cache_dir: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    334\u001b[0m ):\n\u001b[1;32m    335\u001b[0m     has_hf_hub(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 336\u001b[0m     cached_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_file\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeronlg/lib/python3.8/site-packages/huggingface_hub/file_download.py:1349\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m         \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1350\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1351\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1352\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1353\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhead_call_error\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;66;03m# From now on, etag and commit_hash are not None.\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m etag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metag must have been retrieved from server\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "model, _, transform = create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-B-32\",\n",
    "  pretrained=\"laion2B-s13B-b90k\",\n",
    "  device='cuda',\n",
    "  cache_dir=ZERONLG_HOME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9549cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2990/2990 [36:38<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msrvtt en [{'image_id': 7010, 'caption': 'how to download free mp 3 from youtube to your computer '}, {'image_id': 7011, 'caption': 'pink color in english '}, {'image_id': 7012, 'caption': 'watch this video of ed sheeran on the today show '}, {'image_id': 7013, 'caption': 'how to use the fv - 1 2 1 1 to decode a set of 2 2 '}, {'image_id': 7014, 'caption': 'the x - factor 2 0 1 5 : who is the new face of the show ? '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [1:07:22<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco en [{'image_id': 391895, 'caption': 'a man on a mountain bike in the huanglong national park , huanglong , china '}, {'image_id': 60623, 'caption': 'a group of people are eating a big bowl of food . one is a woman is one of '}, {'image_id': 483108, 'caption': 'a man is waiting for a train at the train station in the city of luang prabang '}, {'image_id': 384213, 'caption': 'the kitchen is one of the more well - known in the world . '}, {'image_id': 386164, 'caption': 'photo of a collection of wooden kitchen tools '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [21:01<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vatex zh [{'image_id': 27439, 'caption': 'watch this video of a man who is a new day after a 2 1 - year - old '}, {'image_id': 27440, 'caption': 'watch this video of a man who is a black man who is a white man who is a '}, {'image_id': 27441, 'caption': 'watch this baby play with a bucket of water '}, {'image_id': 27442, 'caption': 'a white paint roller is used to paint a wooden wall in a house . - paint roller stock '}, {'image_id': 27443, 'caption': 'how to clean a white tiled floor with a mops '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:43<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickr30k zh [{'image_id': 1009692167, 'caption': 'a new group of new canaan police department k - 9 s is set to be a part '}, {'image_id': 1021439420, 'caption': 'photo : the family is all set up for a day of family fun . this is one of '}, {'image_id': 1032122270, 'caption': 'two english cocker spaniels and a great dane in a field '}, {'image_id': 1043819504, 'caption': 'photo : the first day of the 2 0 1 2 ice age day camp . '}, {'image_id': 1095580424, 'caption': 'a black and white dog is running in a large bag of food . '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:48<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickr30k de [{'image_id': 1007129816, 'caption': 'a man with a hat made out of a 1 2 - pack of 1 2 - pack of '}, {'image_id': 1009434119, 'caption': 'english : a boston is a small english dog . '}, {'image_id': 101362133, 'caption': '2 0 1 2 world cup of karate - day 1 '}, {'image_id': 102617084, 'caption': 'a group of people in a snow day . one is a white man in a red jacket and '}, {'image_id': 10287332, 'caption': 'new home construction - new home construction is a great time to do home projects . we can help '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:43<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickr30k fr [{'image_id': 1007129816, 'caption': 'a man with a hat made out of a 1 2 - pack of 1 2 - pack of '}, {'image_id': 1009434119, 'caption': 'english : a boston is a small english dog . '}, {'image_id': 101362133, 'caption': '2 0 1 2 world cup of karate - day 1 '}, {'image_id': 102617084, 'caption': 'a group of people in a snow day . one is a white man in a red jacket and '}, {'image_id': 10287332, 'caption': 'new home construction - new home construction is a great time to do home projects . we can help '}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import configs\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from zeronlg import CaptionDataset\n",
    "\n",
    "all_datasets = [\n",
    "    'msrvtt',\n",
    "    'coco',\n",
    "    'vatex',\n",
    "    'flickr30k',\n",
    "]\n",
    "all_langs = [\n",
    "    ['en'],\n",
    "    ['en'],\n",
    "    ['zh'],\n",
    "    ['zh', 'de', 'fr'],\n",
    "]\n",
    "\n",
    "generation_kwargs = {\n",
    "    'num_beams': 3,\n",
    "    'min_seq_len': 3,\n",
    "    'seq_len': 20\n",
    "}\n",
    "\n",
    "for tag, langs in zip(all_datasets, all_langs):\n",
    "    for lang in langs:\n",
    "        dataset = CaptionDataset(\n",
    "            vision_root=configs.image_video_root[tag],\n",
    "            ann_rpath=f'{configs.annotation_root}/{tag}/{lang}/test.json',\n",
    "            lang=lang,\n",
    "            return_images=True,\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=1,\n",
    "            sampler=None,\n",
    "            shuffle=False,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for batch in tqdm(loader):\n",
    "            image_ids, images = batch\n",
    "            image = images[0][len(images[0]) // 2] # get the middle frame\n",
    "            im = [transform(image).unsqueeze(0)]\n",
    "            im = torch.stack(im, dim=1).to('cuda')\n",
    "\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                generated = model.generate(im, **generation_kwargs)\n",
    "\n",
    "            caption = open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\")\n",
    "            results.append({\"image_id\": image_ids[0], \"caption\": caption})\n",
    "\n",
    "        print(tag, lang, results[:5])\n",
    "        results_file = os.path.join(save_path, f'coca-b-32_{lang}_{tag}.json')\n",
    "        json.dump(results, open(results_file, 'w'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c71775b9",
   "metadata": {},
   "source": [
    "# Translate CoCa's results with NLLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dec28f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.27.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (4.27.1)\n",
      "Requirement already satisfied: filelock in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.27.1) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.27.1) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.27.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.27.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.27.1) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "# if your `transformers` version is low, e.g., 4.12.5\n",
    "# then you should upgrade it to load the NLLB model\n",
    "!pip install transformers==4.27.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41cffd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "model_name = f\"{ZERONLG_HOME}/{model_name.replace('/', '_')}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37df3821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating output/coca_results/coca-b-32_zh_vatex.json ...\n",
      "save results to output/coca_results/coca-b-32_zh_vatex_NLLB.json\n",
      "translating output/coca_results/coca-b-32_zh_flickr30k.json ...\n",
      "save results to output/coca_results/coca-b-32_zh_flickr30k_NLLB.json\n",
      "translating output/coca_results/coca-b-32_de_flickr30k.json ...\n",
      "save results to output/coca_results/coca-b-32_de_flickr30k_NLLB.json\n",
      "translating output/coca_results/coca-b-32_fr_flickr30k.json ...\n",
      "save results to output/coca_results/coca-b-32_fr_flickr30k_NLLB.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from zeronlg.utils import batch_to_device\n",
    "\n",
    "tag = 'NLLB'\n",
    "mapping = {\n",
    "    'en': 'eng_Latn',\n",
    "    'zh': 'zho_Hans',\n",
    "    'de': 'deu_Latn',\n",
    "    'fr': 'fra_Latn',\n",
    "}\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "tokenizer.src_lang = mapping['en']\n",
    "\n",
    "all_datasets = [\n",
    "    'vatex',\n",
    "    'flickr30k',\n",
    "]\n",
    "all_langs = [\n",
    "    ['zh'],\n",
    "    ['zh', 'de', 'fr'],\n",
    "]\n",
    "\n",
    "for tag, langs in zip(all_datasets, all_langs):\n",
    "    for lang in langs:\n",
    "        src_path = f\"{save_path}/coca-b-32_{lang}_{tag}.json\"\n",
    "        trg_path = f\"{save_path}/coca-b-32_{lang}_{tag}_NLLB.json\"\n",
    "\n",
    "        print(f'translating {src_path} ...')\n",
    "        data = json.load(open(src_path, 'r'))\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        num_batches = len(data) // batch_size\n",
    "        if batch_size * num_batches != len(data):\n",
    "            num_batches += 1\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start, end = i * batch_size, (i + 1) * batch_size\n",
    "            text = [line['caption'] for line in data[start:end]]\n",
    "            encoded_text = tokenizer(text, return_tensors='pt', padding=True)\n",
    "            encoded_text = batch_to_device(encoded_text, device)\n",
    "            generated_tokens = model.generate(\n",
    "                **encoded_text,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[mapping[lang]]\n",
    "            )\n",
    "            res = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            for line, caption in zip(data[start:end], res):\n",
    "                line['caption'] = caption\n",
    "                results.append(line)\n",
    "\n",
    "        print(f'save results to {trg_path}')\n",
    "        with open(trg_path, 'w') as wf:\n",
    "            json.dump(results, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0853fb48-c88a-475f-8d52-37e4e827a4f4",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d343bd9f-8d72-4aa7-bf9d-6fb6f6b7b835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting transformers==4.12.5\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl.metadata (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m335.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (2.31.0)\n",
      "Requirement already satisfied: sacremoses in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (0.0.53)\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.12.5)\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from transformers==4.12.5) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.12.5) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.12.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2023.7.22)\n",
      "Requirement already satisfied: six in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n",
      "Requirement already satisfied: click in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.3.2)\n",
      "Using cached transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.27.1\n",
      "    Uninstalling transformers-4.27.1:\n",
      "      Successfully uninstalled transformers-4.27.1\n",
      "Successfully installed tokenizers-0.10.3 transformers-4.12.5\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.12.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0572dc-b903-4a6d-82b4-3d577134ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-02-25 17:49:59 - results_file: output/coca_results/coca-b-32_en_msrvtt.json\n",
      "2024-02-25 17:49:59 - gt_file: data/annotations/msrvtt/en/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 617049 tokens at 558328.57 tokens per second.\n",
      "PTBTokenizer tokenized 48701 tokens at 144096.82 tokens per second.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 41143, 'reflen': 37148, 'guess': [41143, 38153, 35163, 32174], 'correct': [11513, 2500, 562, 154]}\n",
      "ratio: 1.1075428017658797\n",
      "Bleu_1: 0.280\n",
      "Bleu_2: 0.135\n",
      "Bleu_3: 0.066\n",
      "Bleu_4: 0.034\n",
      "computing METEOR score...\n",
      "METEOR: 0.102\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.215\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.056\n",
      "computing SPICE score...\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n",
      "SPICE evaluation took: 20.23 s\n",
      "SPICE: 0.026\n",
      "Bleu_1: 0.280\n",
      "Bleu_2: 0.135\n",
      "Bleu_3: 0.066\n",
      "Bleu_4: 0.034\n",
      "METEOR: 0.102\n",
      "ROUGE_L: 0.215\n",
      "CIDEr: 0.056\n",
      "SPICE: 0.026\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_en_msrvtt.json --dataset msrvtt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63dbafce-6538-476c-80c6-2a3d21e6f4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-02-25 17:51:25 - results_file: output/coca_results/coca-b-32_en_coco.json\n",
      "2024-02-25 17:51:25 - gt_file: data/annotations/coco/en/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "PTBTokenizer tokenized 307085 tokens at 573385.60 tokens per second.\n",
      "Feb 25, 2024 5:51:26 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: 「 (U+300C, decimal: 12300)\n",
      "PTBTokenizer tokenized 80843 tokens at 189421.10 tokens per second.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 69059, 'reflen': 58042, 'guess': [69059, 64059, 59061, 54080], 'correct': [20370, 5963, 1716, 512]}\n",
      "ratio: 1.1898108266427554\n",
      "Bleu_1: 0.295\n",
      "Bleu_2: 0.166\n",
      "Bleu_3: 0.093\n",
      "Bleu_4: 0.052\n",
      "computing METEOR score...\n",
      "METEOR: 0.112\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.242\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.167\n",
      "computing SPICE score...\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.nustaq.serialization.FSTClazzInfo (file:/home/bangyang/anaconda3/envs/zeronlg/lib/python3.8/site-packages/pycocoevalcap/spice/lib/fst-2.47.jar) to field java.lang.String.value\n",
      "WARNING: Please consider reporting this to the maintainers of org.nustaq.serialization.FSTClazzInfo\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Warning: Nashorn engine is planned to be removed from a future JDK release\n",
      "SPICE evaluation took: 9.405 s\n",
      "SPICE: 0.068\n",
      "Bleu_1: 0.295\n",
      "Bleu_2: 0.166\n",
      "Bleu_3: 0.093\n",
      "Bleu_4: 0.052\n",
      "METEOR: 0.112\n",
      "ROUGE_L: 0.242\n",
      "CIDEr: 0.167\n",
      "SPICE: 0.068\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_en_coco.json --dataset coco --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a5949d-c610-46c6-8200-af895754b89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-02-25 17:52:18 - results_file: output/coca_results/coca-b-32_zh_vatex_NLLB.json\n",
      "2024-02-25 17:52:18 - gt_file: data/annotations/vatex/zh/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Building prefix dict from the default dictionary ...\n",
      "2024-02-25 17:52:18 - Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2024-02-25 17:52:18 - Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.215 seconds.\n",
      "2024-02-25 17:52:19 - Loading model cost 1.215 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2024-02-25 17:52:19 - Prefix dict has been built successfully.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 15917, 'reflen': 15641, 'guess': [15917, 14417, 12918, 11431], 'correct': [3550, 401, 56, 8]}\n",
      "ratio: 1.0176459305670342\n",
      "Bleu_1: 0.223\n",
      "Bleu_2: 0.079\n",
      "Bleu_3: 0.030\n",
      "Bleu_4: 0.012\n",
      "computing METEOR score...\n",
      "METEOR: 0.096\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.164\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.030\n",
      "Bleu_1: 0.223\n",
      "Bleu_2: 0.079\n",
      "Bleu_3: 0.030\n",
      "Bleu_4: 0.012\n",
      "METEOR: 0.096\n",
      "ROUGE_L: 0.164\n",
      "CIDEr: 0.030\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_zh_vatex_NLLB.json --dataset vatex --lang zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46814903-1824-4f62-9cfd-1aa4f6bd2cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-02-25 17:52:53 - results_file: output/coca_results/coca-b-32_zh_flickr30k_NLLB.json\n",
      "2024-02-25 17:52:53 - gt_file: data/annotations/flickr30k/zh/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Building prefix dict from the default dictionary ...\n",
      "2024-02-25 17:52:53 - Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2024-02-25 17:52:53 - Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.243 seconds.\n",
      "2024-02-25 17:52:54 - Loading model cost 1.243 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2024-02-25 17:52:54 - Prefix dict has been built successfully.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 11572, 'reflen': 9613, 'guess': [11572, 10575, 9578, 8627], 'correct': [2631, 476, 103, 18]}\n",
      "ratio: 1.203786539061562\n",
      "Bleu_1: 0.227\n",
      "Bleu_2: 0.101\n",
      "Bleu_3: 0.048\n",
      "Bleu_4: 0.022\n",
      "computing METEOR score...\n",
      "METEOR: 0.107\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.200\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.074\n",
      "Bleu_1: 0.227\n",
      "Bleu_2: 0.101\n",
      "Bleu_3: 0.048\n",
      "Bleu_4: 0.022\n",
      "METEOR: 0.107\n",
      "ROUGE_L: 0.200\n",
      "CIDEr: 0.074\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_zh_flickr30k_NLLB.json --dataset flickr30k --lang zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a678d67-ad4f-43ba-bf3f-60818f8594d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-02-25 17:53:18 - results_file: output/coca_results/coca-b-32_de_flickr30k_NLLB.json\n",
      "2024-02-25 17:53:18 - gt_file: data/annotations/flickr30k/de/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "2024-02-25 17:53:18 - Initializing native server...\n",
      "2024-02-25 17:53:18 - java -Xmx4g -cp \"/data/yb/checkpoints/stanford-corenlp-4.5.2/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000\n",
      "2024-02-25 17:53:18 - Server shell PID: 1668829\n",
      "2024-02-25 17:53:19 - The server is available.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 12443, 'reflen': 10754, 'guess': [12443, 11443, 10443, 9447], 'correct': [2928, 728, 178, 43]}\n",
      "ratio: 1.1570578389435413\n",
      "Bleu_1: 0.235\n",
      "Bleu_2: 0.122\n",
      "Bleu_3: 0.063\n",
      "Bleu_4: 0.033\n",
      "computing METEOR score...\n",
      "METEOR: 0.100\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.205\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.105\n",
      "Bleu_1: 0.235\n",
      "Bleu_2: 0.122\n",
      "Bleu_3: 0.063\n",
      "Bleu_4: 0.033\n",
      "METEOR: 0.100\n",
      "ROUGE_L: 0.205\n",
      "CIDEr: 0.105\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_de_flickr30k_NLLB.json --dataset flickr30k --lang de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e28c3b65-6124-4ec1-9ae3-6ec7f38a4954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-02-25 17:54:22 - results_file: output/coca_results/coca-b-32_fr_flickr30k_NLLB.json\n",
      "2024-02-25 17:54:22 - gt_file: data/annotations/flickr30k/fr/test_gt.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "2024-02-25 17:54:22 - Initializing native server...\n",
      "2024-02-25 17:54:22 - java -Xmx4g -cp \"/data/yb/checkpoints/stanford-corenlp-4.5.2/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000\n",
      "2024-02-25 17:54:22 - Server shell PID: 2092534\n",
      "2024-02-25 17:54:23 - The server is available.\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 14221, 'reflen': 12839, 'guess': [14221, 13221, 12221, 11222], 'correct': [2222, 522, 139, 41]}\n",
      "ratio: 1.1076407819922807\n",
      "Bleu_1: 0.156\n",
      "Bleu_2: 0.079\n",
      "Bleu_3: 0.041\n",
      "Bleu_4: 0.023\n",
      "computing METEOR score...\n",
      "METEOR: 0.078\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.153\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.188\n",
      "Bleu_1: 0.156\n",
      "Bleu_2: 0.079\n",
      "Bleu_3: 0.041\n",
      "Bleu_4: 0.023\n",
      "METEOR: 0.078\n",
      "ROUGE_L: 0.153\n",
      "CIDEr: 0.188\n"
     ]
    }
   ],
   "source": [
    "!python infer_caption.py --results_file output/coca_results/coca-b-32_fr_flickr30k_NLLB.json --dataset flickr30k --lang fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736aac8-ae2c-4012-afc5-1f3e80244c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
