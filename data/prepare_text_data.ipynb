{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVE_PATH = './annotations'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Flickr30k\n",
    "\n",
    "```\n",
    "@article{young2014image,\n",
    "  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},\n",
    "  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},\n",
    "  journal={Transactions of the Association for Computational Linguistics},\n",
    "  volume={2},\n",
    "  pages={67--78},\n",
    "  year={2014},\n",
    "  publisher={MIT Press}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'flickr30k'\n",
    "RELATIVE_PATH_FORMAT = 'flickr30k-images/{}.jpg'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Flickr30k-EN (Karpathy's splits)\n",
    "```\n",
    "@inproceedings{karpathy2015deep,\n",
    "  title={Deep visual-semantic alignments for generating image descriptions},\n",
    "  author={Karpathy, Andrej and Fei-Fei, Li},\n",
    "  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n",
    "  pages={3128--3137},\n",
    "  year={2015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-14 21:00:07--  https://cs.stanford.edu/people/karpathy/deepimagesent/flickr30k.zip\n",
      "正在解析主机 cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
      "正在连接 cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... 已连接。\n",
      "警告: 无法验证 cs.stanford.edu 的由 “CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US” 颁发的证书:\n",
      "  出现了自己签名的证书。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：197966511 (189M) [application/zip]\n",
      "正在保存至: “flickr30k.zip”\n",
      "\n",
      "flickr30k.zip       100%[===================>] 188.79M  5.49MB/s  用时 36s       \n",
      "\n",
      "2023-02-14 21:00:43 (5.30 MB/s) - 已保存 “flickr30k.zip” [197966511/197966511])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cs.stanford.edu/people/karpathy/deepimagesent/flickr30k.zip --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  flickr30k.zip\n",
      "  inflating: flickr30k/dataset.json  \n",
      "  inflating: flickr30k/vgg_feats.mat  \n",
      "  inflating: flickr30k/readme.txt    \n"
     ]
    }
   ],
   "source": [
    "!unzip -o flickr30k.zip\n",
    "!rm flickr30k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('flickr30k/dataset.json', 'r'))['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentids': [0, 1, 2, 3, 4],\n",
       " 'imgid': 0,\n",
       " 'sentences': [{'tokens': ['two',\n",
       "    'young',\n",
       "    'guys',\n",
       "    'with',\n",
       "    'shaggy',\n",
       "    'hair',\n",
       "    'look',\n",
       "    'at',\n",
       "    'their',\n",
       "    'hands',\n",
       "    'while',\n",
       "    'hanging',\n",
       "    'out',\n",
       "    'in',\n",
       "    'the',\n",
       "    'yard'],\n",
       "   'raw': 'Two young guys with shaggy hair look at their hands while hanging out in the yard.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 0},\n",
       "  {'tokens': ['two',\n",
       "    'young',\n",
       "    'white',\n",
       "    'males',\n",
       "    'are',\n",
       "    'outside',\n",
       "    'near',\n",
       "    'many',\n",
       "    'bushes'],\n",
       "   'raw': 'Two young, White males are outside near many bushes.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 1},\n",
       "  {'tokens': ['two',\n",
       "    'men',\n",
       "    'in',\n",
       "    'green',\n",
       "    'shirts',\n",
       "    'are',\n",
       "    'standing',\n",
       "    'in',\n",
       "    'a',\n",
       "    'yard'],\n",
       "   'raw': 'Two men in green shirts are standing in a yard.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 2},\n",
       "  {'tokens': ['a',\n",
       "    'man',\n",
       "    'in',\n",
       "    'a',\n",
       "    'blue',\n",
       "    'shirt',\n",
       "    'standing',\n",
       "    'in',\n",
       "    'a',\n",
       "    'garden'],\n",
       "   'raw': 'A man in a blue shirt standing in a garden.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 3},\n",
       "  {'tokens': ['two', 'friends', 'enjoy', 'time', 'spent', 'together'],\n",
       "   'raw': 'Two friends enjoy time spent together.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 4}],\n",
       " 'split': 'train',\n",
       " 'filename': '1000092795.jpg'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "imageid2captions = defaultdict(list)\n",
    "imageid2split = {}\n",
    "split2imageids = defaultdict(list)\n",
    "for item in data:\n",
    "    imageid = int(item['filename'].split('.')[0])\n",
    "    for sentence in item['sentences']:\n",
    "        caption = sentence['raw']\n",
    "        imageid2captions[imageid].append(caption)\n",
    "    imageid2split[imageid] = item['split']\n",
    "    split2imageids[item['split']].append(imageid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test', 'train', 'val'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(imageid2split.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'en')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "caption_id = 0\n",
    "\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    json_data = []\n",
    "    for image_id in split2imageids[mode]:\n",
    "        if mode == 'train':\n",
    "            for caption in imageid2captions[image_id]:\n",
    "                item = dict(\n",
    "                    image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                    caption=caption,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "        else:\n",
    "            item = dict(\n",
    "                image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                caption=imageid2captions[image_id],\n",
    "                image_id=image_id,\n",
    "            )\n",
    "            json_data.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(json_data, wf)\n",
    "\n",
    "    if mode != 'train':\n",
    "        gt = {\n",
    "            'annotations': [],\n",
    "            'images': [],\n",
    "        }\n",
    "\n",
    "        for item in json_data:\n",
    "            assert isinstance(item['caption'], (list, tuple))\n",
    "            image_id = item['image_id']\n",
    "            for caption in item['caption']:\n",
    "                item = dict(\n",
    "                    image_id=image_id,\n",
    "                    caption=caption,\n",
    "                    id=caption_id,\n",
    "                )\n",
    "                caption_id += 1\n",
    "                gt['annotations'].append(item)\n",
    "            gt['images'].append({'id': image_id})\n",
    "                    \n",
    "        with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "            json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Flickr30k-ZH\n",
    "\n",
    "```\n",
    "@inproceedings{lan2017fluency,\n",
    "  title={Fluency-guided cross-lingual image captioning},\n",
    "  author={Lan, Weiyu and Li, Xirong and Dong, Jianfeng},\n",
    "  booktitle={Proceedings of the 25th ACM international conference on Multimedia},\n",
    "  pages={1549--1557},\n",
    "  year={2017}\n",
    "}\n",
    "```\n",
    "\n",
    "**Note**: We found that Flickr30k-CN does not follow the splits proposed by Karpathy et al. (https://cs.stanford.edu/people/karpathy/deepimagesent/flickr30k.zip). Besides, some images required by Flicrk30k-CN does not exist in Karpathy's splits (31,014 images) but exist in the official splits (31783 images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**please manually operates as follows:**\n",
    "1. Download Flickr30k-CN from https://github.com/li-xirong/cross-lingual-cap;\n",
    "2. Get the file named `seg.flickr30kzhmbosontest.caption.txt`;\n",
    "3. Put the file to the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "zh_imageid2captions = defaultdict(list)\n",
    "zh_data = open('seg.flickr30kzhmbosontest.caption.txt', 'r').read().strip().split('\\n')\n",
    "for line in zh_data:\n",
    "    splits = line.split(' ')\n",
    "    \n",
    "    tag = splits[0]\n",
    "    imageid = int(tag.split('#')[0]) # e.g., 1009692167#zhm#1 -> 1009692167\n",
    "\n",
    "    tokens = [item.split(':')[0] for item in splits[1:]] # e.g., 一个:m -> 一个\n",
    "    caption = ''.join(tokens)\n",
    "\n",
    "    zh_imageid2captions[imageid].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'zh')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "json_data = []\n",
    "for image_id, captions in zh_imageid2captions.items():\n",
    "    item = dict(\n",
    "        image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "        caption=captions,\n",
    "        image_id=image_id,\n",
    "    )\n",
    "    json_data.append(item)\n",
    "\n",
    "with open(os.path.join(save_path, 'test.json'), 'w') as wf:\n",
    "    json.dump(json_data, wf)\n",
    "\n",
    "gt = {\n",
    "    'annotations': [],\n",
    "    'images': [],\n",
    "}\n",
    "\n",
    "caption_id = 0\n",
    "for item in json_data:\n",
    "    assert isinstance(item['caption'], (list, tuple))\n",
    "    image_id = item['image_id']\n",
    "    for caption in item['caption']:\n",
    "        item = dict(\n",
    "            image_id=image_id,\n",
    "            caption=caption,\n",
    "            id=caption_id,\n",
    "        )\n",
    "        caption_id += 1\n",
    "        gt['annotations'].append(item)\n",
    "    gt['images'].append({'id': image_id})\n",
    "            \n",
    "with open(os.path.join(save_path, f'test_gt.json'), 'w') as wf:\n",
    "    json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Flickr30k-DE, Fr, Cs (Multi30K)\n",
    "\n",
    "```\n",
    "@inproceedings{elliott2016multi30k,\n",
    "  title={Multi30K: Multilingual English-German Image Descriptions},\n",
    "  author={Elliott, Desmond and Frank, Stella and Sima’an, Khalil and Specia, Lucia},\n",
    "  booktitle={Proceedings of the 5th Workshop on Vision and Language},\n",
    "  pages={70--74},\n",
    "  year={2016}\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dataset'...\n",
      "remote: Enumerating objects: 313, done.\u001b[K\n",
      "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
      "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
      "remote: Total 313 (delta 17), reused 21 (delta 16), pack-reused 281\u001b[K\n",
      "Receiving objects: 100% (313/313), 18.21 MiB | 16.00 KiB/s, done.\n",
      "Resolving deltas: 100% (69/69), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/multi30k/dataset.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.1 Flickr30k-DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import gzip\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'de')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['train', 'val', 'test_2016']:\n",
    "    images_list = open(f'dataset/data/task2/image_splits/{mode}_images.txt', 'r').read().strip().split('\\n')\n",
    "    captions_list = []\n",
    "    for i in range(5):\n",
    "        data = []\n",
    "        with gzip.open(f'dataset/data/task2/raw/{mode}.{i+1}.de.gz', 'rt', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                caption = line.strip()\n",
    "                data.append(caption)\n",
    "        assert len(data) == len(images_list)\n",
    "        captions_list.append(data)\n",
    "\n",
    "    mode = mode.split('_')[0] # 'test_2016' -> 'test'\n",
    "    \n",
    "    imageid2captions = {}\n",
    "    for fn, captions in zip(images_list, zip(*captions_list)):\n",
    "        image_id = int(fn.split('.')[0])\n",
    "        imageid2captions[image_id] = list(captions)\n",
    "\n",
    "    json_data = []\n",
    "    for image_id, captions in imageid2captions.items():\n",
    "        if mode == 'train':\n",
    "            for caption in captions:\n",
    "                item = dict(\n",
    "                    image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                    caption=caption,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "        else:\n",
    "            item = dict(\n",
    "                image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                caption=captions,\n",
    "                image_id=image_id,\n",
    "            )\n",
    "            json_data.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(json_data, wf)\n",
    "\n",
    "    if mode != 'train':\n",
    "        gt = {\n",
    "            'annotations': [],\n",
    "            'images': [],\n",
    "        }\n",
    "\n",
    "        for item in json_data:\n",
    "            assert isinstance(item['caption'], (list, tuple))\n",
    "            image_id = item['image_id']\n",
    "            for caption in item['caption']:\n",
    "                item = dict(\n",
    "                    image_id=image_id,\n",
    "                    caption=caption,\n",
    "                    id=caption_id,\n",
    "                )\n",
    "                caption_id += 1\n",
    "                gt['annotations'].append(item)\n",
    "            gt['images'].append({'id': image_id})\n",
    "                    \n",
    "        with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "            json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.2 Flickr30k-FR, CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_tokens(data):\n",
    "    new_data = []\n",
    "    for line in data:\n",
    "        # in translation data, `man 's` is wrriten as `man &apos;s`\n",
    "        line = line.replace(\"&apos;\", \"'\")\n",
    "        # in translation data, `\" 92 \"` is wrriten as `&quot; 92 &quot;`\n",
    "        line = line.replace('&quot;', '\"')\n",
    "        new_data.append(line)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import gzip\n",
    "\n",
    "for lang in ['fr', 'cs']:\n",
    "    save_path = os.path.join(SAVE_PATH, DATASET, lang)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    caption_id = 0\n",
    "    for mode in ['train', 'val', 'test_2016_flickr']:\n",
    "        images_list = open(f'dataset/data/task1/image_splits/{mode}.txt', 'r').read().strip().split('\\n')\n",
    "        captions_list = []\n",
    "        for i in range(1):\n",
    "            data = []\n",
    "            with gzip.open(f'dataset/data/task1/raw/{mode}.{lang}.gz', 'rt', encoding='utf8') as f:\n",
    "                for line in f:\n",
    "                    caption = line.strip()\n",
    "                    data.append(caption)\n",
    "\n",
    "            assert len(data) == len(images_list)\n",
    "            data = replace_special_tokens(data)\n",
    "            captions_list.append(data)\n",
    "        \n",
    "        mode = mode.split('_')[0] # 'test_2016_flickr' -> 'test'\n",
    "        \n",
    "        imageid2captions = {}\n",
    "        for fn, captions in zip(images_list, zip(*captions_list)):\n",
    "            image_id = int(fn.split('.')[0])\n",
    "            imageid2captions[image_id] = list(captions)\n",
    "\n",
    "        json_data = []\n",
    "        for image_id, captions in imageid2captions.items():\n",
    "            if mode == 'train':\n",
    "                for caption in captions:\n",
    "                    item = dict(\n",
    "                        image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                        caption=caption,\n",
    "                        image_id=image_id,\n",
    "                    )\n",
    "                    json_data.append(item)\n",
    "            else:\n",
    "                item = dict(\n",
    "                    image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                    caption=captions,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "\n",
    "        with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "            json.dump(json_data, wf)\n",
    "\n",
    "        if mode != 'train':\n",
    "            gt = {\n",
    "                'annotations': [],\n",
    "                'images': [],\n",
    "            }\n",
    "\n",
    "            for item in json_data:\n",
    "                assert isinstance(item['caption'], (list, tuple))\n",
    "                image_id = item['image_id']\n",
    "                for caption in item['caption']:\n",
    "                    item = dict(\n",
    "                        image_id=image_id,\n",
    "                        caption=caption,\n",
    "                        id=caption_id,\n",
    "                    )\n",
    "                    caption_id += 1\n",
    "                    gt['annotations'].append(item)\n",
    "                gt['images'].append({'id': image_id})\n",
    "                        \n",
    "            with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "                json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.1 Flickr30k EN-ZH Pairs\n",
    "\n",
    "**Note**: please run `Section 1.2 Flicrk30k-ZH` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-23 01:22:34--  http://shannon.cs.illinois.edu/DenotationGraph/data/flickr30k.tar.gz\n",
      "正在解析主机 shannon.cs.illinois.edu (shannon.cs.illinois.edu)... 18.220.149.166\n",
      "正在连接 shannon.cs.illinois.edu (shannon.cs.illinois.edu)|18.220.149.166|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：3652513 (3.5M) [application/x-gzip]\n",
      "正在保存至: “flickr30k.tar.gz”\n",
      "\n",
      "flickr30k.tar.gz    100%[===================>]   3.48M  46.7KB/s  用时 53s       \n",
      "\n",
      "2023-03-23 01:23:28 (67.5 KB/s) - 已保存 “flickr30k.tar.gz” [3652513/3652513])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the official flickr30k annotations and get the file named results_20130124.token\n",
    "!wget http://shannon.cs.illinois.edu/DenotationGraph/data/flickr30k.tar.gz --no-check-certificate\n",
    "!tar -xzf flickr30k.tar.gz\n",
    "!rm flickr30k.tar.gz\n",
    "!rm readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "en_imageid2captions = defaultdict(list)\n",
    "official_data = open('results_20130124.token', 'r').read().strip().split('\\n')\n",
    "for line in official_data:\n",
    "    tag, caption = line.split('\\t')\n",
    "    imageid = int(tag.split('.')[0]) # e.g., 1000092795.jpg#0 -> 1000092795\n",
    "    en_imageid2captions[imageid].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1009692167,\n",
       " 1021439420,\n",
       " 1032122270,\n",
       " 1043819504,\n",
       " 1095580424,\n",
       " 11034843,\n",
       " 11214470,\n",
       " 1128230658,\n",
       " 1132772170,\n",
       " 1143882946]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_imageids = list(zh_imageid2captions.keys())\n",
    "zh_imageids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在警车前，一条训练有素的警犬坐在它的警官身旁。',\n",
       " '一名警察站着，身边有一只德国牧羊犬',\n",
       " '一位安保人员带着他的狗正在寻找某些东西',\n",
       " '一名穿着反光背心的军官和他的狗站在他的车前面',\n",
       " '一个警察和一只搜索犬在街上']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_imageid2captions[zh_imageids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An officer in a reflective vest stands at the front of his van with his dog .',\n",
       " 'A trained police dog sits next to his handler in front of the police van .',\n",
       " 'A security man with his watch dog is looking for something .',\n",
       " 'A policeman is standing with a German Shepherd dog .',\n",
       " 'A policeman stops on a street with a search dog .']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_imageid2captions[zh_imageids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(SAVE_PATH, DATASET, 'en-zh')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "en_captions = [en_imageid2captions[image_id][i] for image_id in zh_imageids for i in range(5)]\n",
    "zh_captions = [zh_imageid2captions[image_id][i] for image_id in zh_imageids for i in range(5)]\n",
    "\n",
    "# add a full stop at the end of each sentence\n",
    "for i in range(len(en_captions)):\n",
    "    en_captions[i] = (en_captions[i] + ' .') if not en_captions[i].endswith(' .') else en_captions[i]\n",
    "    zh_captions[i] = (zh_captions[i] + '。') if not zh_captions[i].endswith('。') else zh_captions[i]\n",
    "\n",
    "with open(os.path.join(path, 'test.en'), 'w') as wf:\n",
    "    wf.write('\\n'.join(en_captions))\n",
    "\n",
    "with open(os.path.join(path, 'test.zh'), 'w') as wf:\n",
    "    wf.write('\\n'.join(zh_captions))\n",
    "\n",
    "image_rpaths = [RELATIVE_PATH_FORMAT.format(image_id) for image_id in zh_imageids for _ in range(5)]\n",
    "with open(os.path.join(path, 'test_images.txt'), 'w') as wf:\n",
    "    wf.write('\\n'.join(image_rpaths))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.2 Flickr30k EN-DE, EN-FR, DE-FR Pairs\n",
    "\n",
    "**Note**: please run `Section 1.3 Flicrk30k-DE, FR,CS` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for folder in ['en-de', 'en-fr', 'de-fr']:\n",
    "    path = os.path.join(SAVE_PATH, DATASET, folder)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    for mode in ['train', 'val', 'test_2016_flickr']:\n",
    "        for lang in folder.split('-'):\n",
    "            src = f'dataset/data/task1/tok/{mode}.lc.norm.tok.{lang}'\n",
    "            data = open(src, 'r').read().strip().split('\\n')\n",
    "            data = replace_special_tokens(data)\n",
    "            \n",
    "            trg = os.path.join(path, f'{mode.split(\"_\")[0]}.{lang}')\n",
    "            with open(trg, 'w') as wf:\n",
    "                wf.write('\\n'.join(data))\n",
    "        \n",
    "        image_ids = [item.split('.')[0] for item in open(f'dataset/data/task1/image_splits/{mode}.txt', 'r').read().strip().split('\\n')]\n",
    "        image_rpaths = [RELATIVE_PATH_FORMAT.format(image_id) for image_id in image_ids]\n",
    "\n",
    "        with open(os.path.join(path, f'{mode.split(\"_\")[0]}_images.txt'), 'w') as wf:\n",
    "            wf.write('\\n'.join(image_rpaths))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.3 Flickr30k ZH-DE, ZH-FR Pairs\n",
    "\n",
    "**Note**: please run `Section 1.2 Flicrk30k-ZH` and `Section 1.3 Flicrk30k-DE, FR,CS` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "zh_imageids = set([item['image_id'] for item in json.load(open(os.path.join(SAVE_PATH, DATASET, 'zh', 'test.json'), 'r'))])\n",
    "de_fr_imageids = set([item['image_id'] for item in json.load(open(os.path.join(SAVE_PATH, DATASET, 'de', 'test.json'), 'r'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(zh_imageids), len(de_fr_imageids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zh_imageids & de_fr_imageids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_zh_path = os.path.join(SAVE_PATH, DATASET, 'en-zh')\n",
    "en_captions = open(os.path.join(en_zh_path, 'test.en'), 'r').read().strip().split('\\n')\n",
    "zh_captions = open(os.path.join(en_zh_path, 'test.zh'), 'r').read().strip().split('\\n')\n",
    "en2zh = {item[0].lower(): item[1] for item in zip(*[en_captions, zh_captions])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_captions = open(os.path.join(SAVE_PATH, DATASET, 'en-de', 'test.en'), 'r').read().strip().split('\\n')\n",
    "\n",
    "for i in range(len(en_captions)):\n",
    "    # add a full stop to each sentence of en_captions\n",
    "    en_captions[i] = (en_captions[i] + ' .') if not en_captions[i].endswith(' .') else en_captions[i]\n",
    "\n",
    "de_captions = open(os.path.join(SAVE_PATH, DATASET, 'en-de', 'test.de'), 'r').read().strip().split('\\n')\n",
    "fr_captions = open(os.path.join(SAVE_PATH, DATASET, 'en-fr', 'test.fr'), 'r').read().strip().split('\\n')\n",
    "\n",
    "en2de = {item[0].lower(): item[1] for item in zip(*[en_captions, de_captions])}\n",
    "en2fr = {item[0].lower(): item[1] for item in zip(*[en_captions, fr_captions])}\n",
    "\n",
    "imageids = open(os.path.join(SAVE_PATH, DATASET, 'en-de', 'test_images.txt'), 'r').read().strip().split('\\n')\n",
    "en2imageid = {item[0].lower(): item[1].split('/')[-1].split('.')[0] for item in zip(*[en_captions, imageids])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_en = set(list(en2zh.keys())) & set(list(en2de.keys()))\n",
    "len(common_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trg_lang, trg_map in zip(['de', 'fr'], [en2de, en2fr]):\n",
    "    path = os.path.join(SAVE_PATH, DATASET, f'zh-{trg_lang}')\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(path, 'test.zh'), 'w') as wf:\n",
    "        wf.write('\\n'.join([en2zh[en] for en in common_en]))\n",
    "    \n",
    "    with open(os.path.join(path, f'test.{trg_lang}'), 'w') as wf:\n",
    "        wf.write('\\n'.join([trg_map[en] for en in common_en]))\n",
    "    \n",
    "    image_rpaths = [RELATIVE_PATH_FORMAT.format(en2imageid[en]) for en in common_en]\n",
    "    with open(os.path.join(path, 'test_images.txt'), 'w') as wf:\n",
    "        wf.write('\\n'.join(image_rpaths))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MS-COCO\n",
    "\n",
    "```\n",
    "@article{chen2015microsoft,\n",
    "  title={Microsoft coco captions: Data collection and evaluation server},\n",
    "  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\\'a}r, Piotr and Zitnick, C Lawrence},\n",
    "  journal={arXiv preprint arXiv:1504.00325},\n",
    "  year={2015}\n",
    "}\n",
    "@inproceedings{lin2014microsoft,\n",
    "  title={Microsoft coco: Common objects in context},\n",
    "  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\\'a}r, Piotr and Zitnick, C Lawrence},\n",
    "  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},\n",
    "  pages={740--755},\n",
    "  year={2014},\n",
    "  organization={Springer}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'coco'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 COCO-EN (Karpathy's splits)\n",
    "\n",
    "```\n",
    "@inproceedings{karpathy2015deep,\n",
    "  title={Deep visual-semantic alignments for generating image descriptions},\n",
    "  author={Karpathy, Andrej and Fei-Fei, Li},\n",
    "  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n",
    "  pages={3128--3137},\n",
    "  year={2015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-23 14:19:46--  https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
      "正在解析主机 cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
      "正在连接 cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... 已连接。\n",
      "警告: 无法验证 cs.stanford.edu 的由 “CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US” 颁发的证书:\n",
      "  出现了自己签名的证书。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：36745453 (35M) [application/zip]\n",
      "正在保存至: “caption_datasets.zip”\n",
      "\n",
      "caption_datasets.zi 100%[===================>]  35.04M  4.20MB/s  用时 19s       \n",
      "\n",
      "2023-03-23 14:20:06 (1.83 MB/s) - 已保存 “caption_datasets.zip” [36745453/36745453])\n",
      "\n",
      "Archive:  caption_datasets.zip\n",
      "  inflating: dataset_coco.json       \n",
      "  inflating: dataset_flickr30k.json  \n",
      "  inflating: dataset_flickr8k.json   \n"
     ]
    }
   ],
   "source": [
    "!wget https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip --no-check-certificate\n",
    "!unzip caption_datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('dataset_coco.json', 'r'))['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': 'val2014',\n",
       " 'sentids': [770337, 771687, 772707, 776154, 781998],\n",
       " 'filename': 'COCO_val2014_000000391895.jpg',\n",
       " 'imgid': 0,\n",
       " 'split': 'test',\n",
       " 'sentences': [{'tokens': ['a',\n",
       "    'man',\n",
       "    'with',\n",
       "    'a',\n",
       "    'red',\n",
       "    'helmet',\n",
       "    'on',\n",
       "    'a',\n",
       "    'small',\n",
       "    'moped',\n",
       "    'on',\n",
       "    'a',\n",
       "    'dirt',\n",
       "    'road'],\n",
       "   'raw': 'A man with a red helmet on a small moped on a dirt road. ',\n",
       "   'imgid': 0,\n",
       "   'sentid': 770337},\n",
       "  {'tokens': ['man',\n",
       "    'riding',\n",
       "    'a',\n",
       "    'motor',\n",
       "    'bike',\n",
       "    'on',\n",
       "    'a',\n",
       "    'dirt',\n",
       "    'road',\n",
       "    'on',\n",
       "    'the',\n",
       "    'countryside'],\n",
       "   'raw': 'Man riding a motor bike on a dirt road on the countryside.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 771687},\n",
       "  {'tokens': ['a',\n",
       "    'man',\n",
       "    'riding',\n",
       "    'on',\n",
       "    'the',\n",
       "    'back',\n",
       "    'of',\n",
       "    'a',\n",
       "    'motorcycle'],\n",
       "   'raw': 'A man riding on the back of a motorcycle.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 772707},\n",
       "  {'tokens': ['a',\n",
       "    'dirt',\n",
       "    'path',\n",
       "    'with',\n",
       "    'a',\n",
       "    'young',\n",
       "    'person',\n",
       "    'on',\n",
       "    'a',\n",
       "    'motor',\n",
       "    'bike',\n",
       "    'rests',\n",
       "    'to',\n",
       "    'the',\n",
       "    'foreground',\n",
       "    'of',\n",
       "    'a',\n",
       "    'verdant',\n",
       "    'area',\n",
       "    'with',\n",
       "    'a',\n",
       "    'bridge',\n",
       "    'and',\n",
       "    'a',\n",
       "    'background',\n",
       "    'of',\n",
       "    'cloud',\n",
       "    'wreathed',\n",
       "    'mountains'],\n",
       "   'raw': 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ',\n",
       "   'imgid': 0,\n",
       "   'sentid': 776154},\n",
       "  {'tokens': ['a',\n",
       "    'man',\n",
       "    'in',\n",
       "    'a',\n",
       "    'red',\n",
       "    'shirt',\n",
       "    'and',\n",
       "    'a',\n",
       "    'red',\n",
       "    'hat',\n",
       "    'is',\n",
       "    'on',\n",
       "    'a',\n",
       "    'motorcycle',\n",
       "    'on',\n",
       "    'a',\n",
       "    'hill',\n",
       "    'side'],\n",
       "   'raw': 'A man in a red shirt and a red hat is on a motorcycle on a hill side.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 781998}],\n",
       " 'cocoid': 391895}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "imageid2rpath = {}\n",
    "imageid2captions = defaultdict(list)\n",
    "split2imageids = defaultdict(list)\n",
    "for item in data:\n",
    "    imageid = int(item['filename'].split('.')[0].split('_')[-1])\n",
    "    for sentence in item['sentences']:\n",
    "        caption = sentence['raw']\n",
    "        imageid2captions[imageid].append(caption)\n",
    "    split2imageids[item['split']].append(imageid)\n",
    "    imageid2rpath[imageid] =  os.path.join(item['filepath'], item['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 5000\n",
      "restval 30504\n",
      "val 5000\n",
      "train 82783\n"
     ]
    }
   ],
   "source": [
    "for k, v in split2imageids.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'en')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    json_data = []\n",
    "\n",
    "    if mode == 'train':\n",
    "        image_ids_of_this_split = split2imageids['restval'] + split2imageids[mode]\n",
    "    else:\n",
    "        image_ids_of_this_split = split2imageids[mode]\n",
    "\n",
    "    for image_id in image_ids_of_this_split:\n",
    "        if mode == 'train':\n",
    "            for caption in imageid2captions[image_id]:\n",
    "                item = dict(\n",
    "                    image=imageid2rpath[image_id],\n",
    "                    caption=caption,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "        else:\n",
    "            item = dict(\n",
    "                image=imageid2rpath[image_id],\n",
    "                caption=imageid2captions[image_id],\n",
    "                image_id=image_id,\n",
    "            )\n",
    "            json_data.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(json_data, wf)\n",
    "\n",
    "    if mode != 'train':\n",
    "        gt = {\n",
    "            'annotations': [],\n",
    "            'images': [],\n",
    "        }\n",
    "\n",
    "        for item in json_data:\n",
    "            assert isinstance(item['caption'], (list, tuple))\n",
    "            image_id = item['image_id']\n",
    "            for caption in item['caption']:\n",
    "                item = dict(\n",
    "                    image_id=image_id,\n",
    "                    caption=caption,\n",
    "                    id=caption_id,\n",
    "                )\n",
    "                caption_id += 1\n",
    "                gt['annotations'].append(item)\n",
    "            gt['images'].append({'id': image_id})\n",
    "                    \n",
    "        with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "            json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 COCO-JA\n",
    "\n",
    "```\n",
    "@InProceedings{Yoshikawa2017,\n",
    "  title     = {STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset},\n",
    "  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},\n",
    "  month     = {July},\n",
    "  year      = {2017},\n",
    "  address   = {Vancouver, Canada},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {417--421},\n",
    "  url       = {http://www.aclweb.org/anthology/P17-2066}\n",
    "}\n",
    "```\n",
    "\n",
    "Please run `Section 2.1 COCO-EN` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-23 15:06:17--  https://github.com/STAIR-Lab-CIT/STAIR-captions/raw/master/stair_captions_v1.2.tar.gz\n",
      "正在解析主机 github.com (github.com)... 140.82.113.4\n",
      "正在连接 github.com (github.com)|140.82.113.4|:443... 已连接。\n",
      "警告: 无法验证 github.com 的由 “CN=DigiCert TLS Hybrid ECC SHA384 2020 CA1,O=DigiCert Inc,C=US” 颁发的证书:\n",
      "  无法本地校验颁发者的权限。\n",
      "已发出 HTTP 请求，正在等待回应... 302 Found\n",
      "位置：https://raw.githubusercontent.com/STAIR-Lab-CIT/STAIR-captions/master/stair_captions_v1.2.tar.gz [跟随至新的 URL]\n",
      "--2023-03-23 15:06:18--  https://raw.githubusercontent.com/STAIR-Lab-CIT/STAIR-captions/master/stair_captions_v1.2.tar.gz\n",
      "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.96.133\n",
      "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|199.232.96.133|:443... ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/STAIR-Lab-CIT/STAIR-captions/raw/master/stair_captions_v1.2.tar.gz --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x stair_captions_v1.2_train.json\n",
      "x stair_captions_v1.2_train_tokenized.json\n",
      "x stair_captions_v1.2_val.json\n",
      "x stair_captions_v1.2_val_tokenized.json\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf stair_captions_v1.2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'images', 'licenses', 'annotations'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "train_data = json.load(open('stair_captions_v1.2_train.json', 'r'))\n",
    "val_data = json.load(open('stair_captions_v1.2_val.json', 'r'))\n",
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 203312, 'id': 3, 'caption': '山の中を赤い電車が走っている'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'license': 5,\n",
       " 'file_name': 'COCO_train2014_000000057870.jpg',\n",
       " 'coco_url': 'http://mscoco.org/images/57870',\n",
       " 'height': 480,\n",
       " 'width': 640,\n",
       " 'date_captured': '2013-11-14 16:28:13',\n",
       " 'flickr_url': 'http://farm4.staticflickr.com/3153/2970773875_164f0c0b83_z.jpg',\n",
       " 'id': 57870}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "imageid2captions = defaultdict(list)\n",
    "for item in train_data['annotations'] + val_data['annotations']:\n",
    "    imageid2captions[item['image_id']].append(item['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'ja')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    json_data = []\n",
    "    # split2imageids is obtained by Section 2.1, so COCO-JA shares the same Karpathy's splits\n",
    "    if mode == 'train':\n",
    "        image_ids_of_this_split = split2imageids['restval'] + split2imageids[mode]\n",
    "    else:\n",
    "        image_ids_of_this_split = split2imageids[mode]\n",
    "\n",
    "    for image_id in image_ids_of_this_split:\n",
    "        if mode == 'train':\n",
    "            for caption in imageid2captions[image_id]:\n",
    "                item = dict(\n",
    "                    image=imageid2rpath[image_id],\n",
    "                    caption=caption,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "        else:\n",
    "            item = dict(\n",
    "                image=imageid2rpath[image_id],\n",
    "                caption=imageid2captions[image_id],\n",
    "                image_id=image_id,\n",
    "            )\n",
    "            json_data.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(json_data, wf)\n",
    "\n",
    "    if mode != 'train':\n",
    "        gt = {\n",
    "            'annotations': [],\n",
    "            'images': [],\n",
    "        }\n",
    "\n",
    "        for item in json_data:\n",
    "            assert isinstance(item['caption'], (list, tuple))\n",
    "            image_id = item['image_id']\n",
    "            for caption in item['caption']:\n",
    "                item = dict(\n",
    "                    image_id=image_id,\n",
    "                    caption=caption,\n",
    "                    id=caption_id,\n",
    "                )\n",
    "                caption_id += 1\n",
    "                gt['annotations'].append(item)\n",
    "            gt['images'].append({'id': image_id})\n",
    "                    \n",
    "        with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "            json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MSR-VTT\n",
    "\n",
    "```\n",
    "@inproceedings{xu2016msr,\n",
    "  title={Msr-vtt: A large video description dataset for bridging video and language},\n",
    "  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},\n",
    "  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n",
    "  pages={5288--5296},\n",
    "  year={2016}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'msrvtt'\n",
    "RELATIVE_PATH_FORMAT = 'all_videos/video{vid}.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "json_path = os.path.join(save_path, 'videodatainfo_2016.json')\n",
    "\n",
    "if not os.path.exists(json_path):\n",
    "    # the official url is http://ms-multimedia-challenge.com/2016, but the website is out-of-date\n",
    "    url = \"https://github.com/ybCliff/VideoCaptioning/releases/download/v1.0/videodatainfo_2016.json\"\n",
    "    wget.download(url, json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'en')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "data = json.load(open(json_path, 'r'))\n",
    "splits = defaultdict(list)\n",
    "\n",
    "for item in data['videos']:\n",
    "    # 'video1000' -> 1000\n",
    "    vid = int(item['video_id'][5:])\n",
    "    splits[item['split']].append(vid)\n",
    "\n",
    "for k in splits.keys():\n",
    "    splits[k] = sorted(splits[k])\n",
    "\n",
    "splits['val'] = splits.pop('validate')\n",
    "\n",
    "vid2captions = defaultdict(list)\n",
    "\n",
    "for item in data['sentences']:\n",
    "    vid = int(item['video_id'][5:])\n",
    "    vid2captions[vid].append(item['caption'])\n",
    "\n",
    "train_items = []\n",
    "for vid in splits['train']:\n",
    "    for caption in vid2captions[vid]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid), # we only record the relative path of a video\n",
    "            caption=caption,\n",
    "            image_id=vid,\n",
    "        )\n",
    "        train_items.append(item)\n",
    "\n",
    "with open(os.path.join(save_path, 'train.json'), 'w') as wf:\n",
    "    json.dump(train_items, wf)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['val', 'test']:\n",
    "    items = []\n",
    "    for vid in splits[mode]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid),\n",
    "            caption=vid2captions[vid],\n",
    "            image_id=vid,\n",
    "        )\n",
    "        items.append(item)\n",
    "    \n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(items, wf)\n",
    "    \n",
    "    gt = {\n",
    "        'annotations': [],\n",
    "        'images': [{'id': vid} for vid in splits[mode]],\n",
    "    }\n",
    "\n",
    "    for vid in splits[mode]:\n",
    "        for caption in vid2captions[vid]:\n",
    "            item = dict(\n",
    "                image_id=vid,\n",
    "                caption=caption,\n",
    "                id=caption_id,\n",
    "            )\n",
    "            caption_id += 1\n",
    "            gt['annotations'].append(item)\n",
    "    \n",
    "    with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "        json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. VATEX\n",
    "```\n",
    "@inproceedings{wang2019vatex,\n",
    "  title={Vatex: A large-scale, high-quality multilingual dataset for video-and-language research},\n",
    "  author={Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},\n",
    "  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n",
    "  pages={4581--4591},\n",
    "  year={2019}\n",
    "}\n",
    "```\n",
    "\n",
    "**Notes: Some videos of this dataset can not be downloaded any more; We use the dataset identical to the following paper:**\n",
    "```\n",
    "@inproceedings{yang2022clip,\n",
    "  title={CLIP Meets Video Captioning: Concept-Aware Representation Learning Does Matter},\n",
    "  author={Yang, Bang and Zhang, Tong and Zou, Yuexian},\n",
    "  booktitle={Pattern Recognition and Computer Vision: 5th Chinese Conference, PRCV 2022, Shenzhen, China, November 4--7, 2022, Proceedings, Part I},\n",
    "  pages={368--381},\n",
    "  year={2022},\n",
    "}\n",
    "```\n",
    "url: https://github.com/yangbang18/CLIP-Captioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'vatex'\n",
    "RELATIVE_PATH_FORMAT = 'all_videos/video{vid}.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget \n",
    "\n",
    "root = os.path.join(SAVE_PATH, DATASET)\n",
    "os.makedirs(root, exist_ok=True)\n",
    "\n",
    "base_url = \"https://eric-xw.github.io/vatex-website/data/\"\n",
    "\n",
    "for filename in ['vatex_training_v1.0.json', 'vatex_validation_v1.0.json', 'vatex_public_test_english_v1.1.json']:\n",
    "    now_path = os.path.join(root, filename)\n",
    "    now_url = base_url + filename\n",
    "    if not os.path.exists(now_path):\n",
    "        wget.download(now_url, out=now_path)\n",
    "\n",
    "base_url = \"https://raw.githubusercontent.com/yangbang18/CLIP-Captioner/master/data/\"\n",
    "for filename in ['vatex_mapping.txt', 'vatex_existed_videos.txt']:\n",
    "    now_path = os.path.join(root, filename)\n",
    "    now_url = base_url + filename\n",
    "    if not os.path.exists(now_path):\n",
    "        wget.download(now_url, out=now_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 VATEX-EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `train` split has 25006 videos, 985 of 25991 videos are unaccessible!\n",
      "The `val` split has 2893 videos, 107 of 3000 videos are unaccessible!\n",
      "The `test` split has 5792 videos, 208 of 6000 videos are unaccessible!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'en')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "lines = open(os.path.join(root, 'vatex_mapping.txt'), 'r').read().strip().split('\\n')\n",
    "id2vid = {}\n",
    "for line in lines:\n",
    "    id, vid = line.split(' ')\n",
    "    id2vid[id] = vid # e.g., Ptf_2VRj-V0_000122_000132 -> video0\n",
    "\n",
    "existed_videos = open(os.path.join(root, 'vatex_existed_videos.txt'), 'r').read().strip().split('\\n')\n",
    "existed_videos = set([int(item[5:]) for item in existed_videos])\n",
    "\n",
    "splits = defaultdict(list)\n",
    "vid2captions = {}\n",
    "vid2captions_zh = {}\n",
    "for fn, split in zip(\n",
    "    ['vatex_training_v1.0.json', 'vatex_validation_v1.0.json', 'vatex_public_test_english_v1.1.json'], \n",
    "    ['train', 'val', 'test']):\n",
    "    data = json.load(open(os.path.join(root, fn), 'r'))\n",
    "\n",
    "    failed_count = 0\n",
    "    for item in data:\n",
    "        # Ptf_2VRj-V0_000122_000132 -> video0\n",
    "        vid = id2vid[item['videoID']]\n",
    "        # video0 -> 0\n",
    "        vid = int(vid[5:])\n",
    "        if vid not in existed_videos:\n",
    "            # we do not use the annotations of those unaccessible videos\n",
    "            failed_count += 1\n",
    "            continue\n",
    "\n",
    "        splits[split].append(vid)\n",
    "        vid2captions[vid] = item['enCap']\n",
    "        if split == 'test':\n",
    "            assert 'chCap' not in item\n",
    "        else:\n",
    "            vid2captions_zh[vid] = item['chCap']\n",
    "    \n",
    "    print(f'The `{split}` split has {len(data) - failed_count} videos, {failed_count} of {len(data)} videos are unaccessible!')\n",
    "\n",
    "for k in splits.keys():\n",
    "    splits[k] = sorted(splits[k])\n",
    "\n",
    "train_items = []\n",
    "for vid in splits['train']:\n",
    "    for caption in vid2captions[vid]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid), # we only record the relative path of a video\n",
    "            caption=caption,\n",
    "            image_id=vid,\n",
    "        )\n",
    "        train_items.append(item)\n",
    "\n",
    "with open(os.path.join(save_path, 'train.json'), 'w') as wf:\n",
    "    json.dump(train_items, wf)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['val', 'test']:\n",
    "    items = []\n",
    "    for vid in splits[mode]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid),\n",
    "            caption=vid2captions[vid],\n",
    "            image_id=vid,\n",
    "        )\n",
    "        items.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(items, wf)\n",
    "    \n",
    "    gt = {\n",
    "        'annotations': [],\n",
    "        'images': [{'id': vid} for vid in splits[mode]],\n",
    "    }\n",
    "\n",
    "    for vid in splits[mode]:\n",
    "        for caption in vid2captions[vid]:\n",
    "            item = dict(\n",
    "                image_id=vid,\n",
    "                caption=caption,\n",
    "                id=caption_id,\n",
    "            )\n",
    "            caption_id += 1\n",
    "            gt['annotations'].append(item)\n",
    "    \n",
    "    with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "        json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 VATEX-ZH\n",
    "\n",
    "In the following, we provide the splits of VATEX-ZH identical to the paper:\n",
    "```\n",
    "@article{Yang2023ZeroNLG,\n",
    "   title={ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation},\n",
    "   author={Yang, Bang and Liu, Fenglin and Zou, Yuexian and Wu, Xian and Wang, Yaowei and Clifton, David A.},\n",
    "   journal={arXiv preprint arXiv:2303.06458}\n",
    "   year={2023}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `train` split has 25006 videos, 985 of 25991 videos are unaccessible!\n",
      "The `val` split has 2893 videos, 107 of 3000 videos are unaccessible!\n",
      "Constructing the test split by myself\n",
      "Now, train: val: test == 25006: 1393: 1500\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'zh')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "vid2captions_zh = {}\n",
    "for fn, split in zip(\n",
    "    ['vatex_training_v1.0.json', 'vatex_validation_v1.0.json', 'vatex_public_test_english_v1.1.json'], \n",
    "    ['train', 'val', 'test']):\n",
    "    data = json.load(open(os.path.join(root, fn), 'r'))\n",
    "\n",
    "    failed_count = 0\n",
    "    for item in data:\n",
    "        # Ptf_2VRj-V0_000122_000132 -> video0\n",
    "        vid = id2vid[item['videoID']]\n",
    "        # video0 -> 0\n",
    "        vid = int(vid[5:])\n",
    "        if vid not in existed_videos:\n",
    "            # we do not use the annotations of those unaccessible videos\n",
    "            failed_count += 1\n",
    "            continue\n",
    "\n",
    "        if split == 'test':\n",
    "            assert 'chCap' not in item\n",
    "        else:\n",
    "            vid2captions_zh[vid] = item['chCap']\n",
    "    \n",
    "    if split != 'test':\n",
    "        print(f'The `{split}` split has {len(data) - failed_count} videos, {failed_count} of {len(data)} videos are unaccessible!')\n",
    "\n",
    "train_items_zh = []\n",
    "for vid in splits['train']:\n",
    "    for caption in vid2captions_zh[vid]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid), # we only record the relative path of a video\n",
    "            caption=caption,\n",
    "            image_id=vid,\n",
    "        )\n",
    "        train_items_zh.append(item)\n",
    "\n",
    "with open(os.path.join(save_path, 'train.json'), 'w') as wf:\n",
    "    json.dump(train_items_zh, wf)\n",
    "\n",
    "\n",
    "print('Constructing the test split by myself')\n",
    "splits_zh = copy.deepcopy(splits)\n",
    "splits_zh['test'] = splits_zh['val'][-1500:]\n",
    "splits_zh['val'] = splits_zh['val'][:-1500]\n",
    "\n",
    "print(f'Now, train: val: test == {len(splits_zh[\"train\"])}: {len(splits_zh[\"val\"])}: {len(splits_zh[\"test\"])}')\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['val', 'test']:\n",
    "    items = []\n",
    "    for vid in splits_zh[mode]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid),\n",
    "            caption=vid2captions_zh[vid],\n",
    "            image_id=vid,\n",
    "        )\n",
    "        items.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(items, wf)\n",
    "    \n",
    "    gt = {\n",
    "        'annotations': [],\n",
    "        'images': [{'id': vid} for vid in splits_zh[mode]],\n",
    "    }\n",
    "\n",
    "    for vid in splits_zh[mode]:\n",
    "        for caption in vid2captions_zh[vid]:\n",
    "            item = dict(\n",
    "                image_id=vid,\n",
    "                caption=caption,\n",
    "                id=caption_id,\n",
    "            )\n",
    "            caption_id += 1\n",
    "            gt['annotations'].append(item)\n",
    "    \n",
    "    with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "        json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Subsets for Semi-Supervised Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "SAVE_PATH = './annotations'\n",
    "\n",
    "def run(dataset='coco', lang='en', ratios=[0.01, 0.1, 1, 10], seeds=[0, 1, 2]):\n",
    "    root = os.path.join(SAVE_PATH, dataset, lang)\n",
    "    train_file = os.path.join(root, 'train.json')\n",
    "    assert os.path.exists(train_file), train_file\n",
    "\n",
    "    print('### load json path from', train_file)\n",
    "    data = json.load(open(train_file, 'r'))\n",
    "\n",
    "    id2item = defaultdict(list)\n",
    "    for item in data:\n",
    "        id2item[item['image_id']].append(item)\n",
    "    \n",
    "    ids = sorted(list(id2item.keys()))\n",
    "\n",
    "    print(f'### there are {len(id2item)} unique images/videos, {len(data)} vision-caption pairs')\n",
    "\n",
    "    save_path = os.path.join(root, 'subsets')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    for ratio in ratios:\n",
    "        n_unique_images = int(len(id2item) * ratio / 100)\n",
    "        if n_unique_images < 1:\n",
    "            print(f'{ratio} is not applicible')\n",
    "            continue\n",
    "\n",
    "        print(f'--- generating a training subset of {ratio}% ({n_unique_images}) unique images/videos')\n",
    "\n",
    "        for seed in seeds:\n",
    "            json_path = os.path.join(save_path, f'{ratio}%_{seed}.json')\n",
    "            if os.path.exists(json_path):\n",
    "                print(json_path, 'exists')\n",
    "                continue\n",
    "            \n",
    "            random.seed(seed)\n",
    "            \n",
    "            this_ids = random.sample(ids, n_unique_images)\n",
    "            this_data = []\n",
    "            for this_id in this_ids:\n",
    "                this_data.extend(id2item[this_id])\n",
    "\n",
    "            print(json_path)\n",
    "            with open(json_path, 'w') as wf:\n",
    "                json.dump(this_data, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### load json path from ./annotations/coco/en/train.json\n",
      "### there are 113287 unique images/videos, 1024649 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (11) unique images/videos\n",
      "./annotations/coco/en/subsets/0.01%_0.json exists\n",
      "./annotations/coco/en/subsets/0.01%_1.json exists\n",
      "./annotations/coco/en/subsets/0.01%_2.json exists\n",
      "--- generating a training subset of 0.1% (113) unique images/videos\n",
      "./annotations/coco/en/subsets/0.1%_0.json exists\n",
      "./annotations/coco/en/subsets/0.1%_1.json exists\n",
      "./annotations/coco/en/subsets/0.1%_2.json exists\n",
      "--- generating a training subset of 1% (1132) unique images/videos\n",
      "./annotations/coco/en/subsets/1%_0.json exists\n",
      "./annotations/coco/en/subsets/1%_1.json exists\n",
      "./annotations/coco/en/subsets/1%_2.json exists\n",
      "--- generating a training subset of 10% (11328) unique images/videos\n",
      "./annotations/coco/en/subsets/10%_0.json exists\n",
      "./annotations/coco/en/subsets/10%_1.json exists\n",
      "./annotations/coco/en/subsets/10%_2.json exists\n",
      "### load json path from ./annotations/coco/ja/train.json\n",
      "### there are 113287 unique images/videos, 1023995 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (11) unique images/videos\n",
      "./annotations/coco/ja/subsets/0.01%_0.json\n",
      "./annotations/coco/ja/subsets/0.01%_1.json\n",
      "./annotations/coco/ja/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (113) unique images/videos\n",
      "./annotations/coco/ja/subsets/0.1%_0.json\n",
      "./annotations/coco/ja/subsets/0.1%_1.json\n",
      "./annotations/coco/ja/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (1132) unique images/videos\n",
      "./annotations/coco/ja/subsets/1%_0.json\n",
      "./annotations/coco/ja/subsets/1%_1.json\n",
      "./annotations/coco/ja/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (11328) unique images/videos\n",
      "./annotations/coco/ja/subsets/10%_0.json\n",
      "./annotations/coco/ja/subsets/10%_1.json\n",
      "./annotations/coco/ja/subsets/10%_2.json\n"
     ]
    }
   ],
   "source": [
    "run('coco', 'en')\n",
    "run('coco', 'ja') # not used at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### load json path from ./annotations/flickr30k/en/train.json\n",
      "### there are 29000 unique images/videos, 145000 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/flickr30k/en/subsets/0.01%_0.json\n",
      "./annotations/flickr30k/en/subsets/0.01%_1.json\n",
      "./annotations/flickr30k/en/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (29) unique images/videos\n",
      "./annotations/flickr30k/en/subsets/0.1%_0.json\n",
      "./annotations/flickr30k/en/subsets/0.1%_1.json\n",
      "./annotations/flickr30k/en/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (290) unique images/videos\n",
      "./annotations/flickr30k/en/subsets/1%_0.json\n",
      "./annotations/flickr30k/en/subsets/1%_1.json\n",
      "./annotations/flickr30k/en/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2900) unique images/videos\n",
      "./annotations/flickr30k/en/subsets/10%_0.json\n",
      "./annotations/flickr30k/en/subsets/10%_1.json\n",
      "./annotations/flickr30k/en/subsets/10%_2.json\n",
      "### load json path from ./annotations/flickr30k/de/train.json\n",
      "### there are 29000 unique images/videos, 145000 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/flickr30k/de/subsets/0.01%_0.json exists\n",
      "./annotations/flickr30k/de/subsets/0.01%_1.json exists\n",
      "./annotations/flickr30k/de/subsets/0.01%_2.json exists\n",
      "--- generating a training subset of 0.1% (29) unique images/videos\n",
      "./annotations/flickr30k/de/subsets/0.1%_0.json exists\n",
      "./annotations/flickr30k/de/subsets/0.1%_1.json exists\n",
      "./annotations/flickr30k/de/subsets/0.1%_2.json exists\n",
      "--- generating a training subset of 1% (290) unique images/videos\n",
      "./annotations/flickr30k/de/subsets/1%_0.json exists\n",
      "./annotations/flickr30k/de/subsets/1%_1.json exists\n",
      "./annotations/flickr30k/de/subsets/1%_2.json exists\n",
      "--- generating a training subset of 10% (2900) unique images/videos\n",
      "./annotations/flickr30k/de/subsets/10%_0.json exists\n",
      "./annotations/flickr30k/de/subsets/10%_1.json exists\n",
      "./annotations/flickr30k/de/subsets/10%_2.json exists\n",
      "### load json path from ./annotations/flickr30k/fr/train.json\n",
      "### there are 29000 unique images/videos, 29000 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/flickr30k/fr/subsets/0.01%_0.json\n",
      "./annotations/flickr30k/fr/subsets/0.01%_1.json\n",
      "./annotations/flickr30k/fr/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (29) unique images/videos\n",
      "./annotations/flickr30k/fr/subsets/0.1%_0.json\n",
      "./annotations/flickr30k/fr/subsets/0.1%_1.json\n",
      "./annotations/flickr30k/fr/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (290) unique images/videos\n",
      "./annotations/flickr30k/fr/subsets/1%_0.json\n",
      "./annotations/flickr30k/fr/subsets/1%_1.json\n",
      "./annotations/flickr30k/fr/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2900) unique images/videos\n",
      "./annotations/flickr30k/fr/subsets/10%_0.json\n",
      "./annotations/flickr30k/fr/subsets/10%_1.json\n",
      "./annotations/flickr30k/fr/subsets/10%_2.json\n",
      "### load json path from ./annotations/flickr30k/cs/train.json\n",
      "### there are 29000 unique images/videos, 29000 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/flickr30k/cs/subsets/0.01%_0.json\n",
      "./annotations/flickr30k/cs/subsets/0.01%_1.json\n",
      "./annotations/flickr30k/cs/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (29) unique images/videos\n",
      "./annotations/flickr30k/cs/subsets/0.1%_0.json\n",
      "./annotations/flickr30k/cs/subsets/0.1%_1.json\n",
      "./annotations/flickr30k/cs/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (290) unique images/videos\n",
      "./annotations/flickr30k/cs/subsets/1%_0.json\n",
      "./annotations/flickr30k/cs/subsets/1%_1.json\n",
      "./annotations/flickr30k/cs/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2900) unique images/videos\n",
      "./annotations/flickr30k/cs/subsets/10%_0.json\n",
      "./annotations/flickr30k/cs/subsets/10%_1.json\n",
      "./annotations/flickr30k/cs/subsets/10%_2.json\n"
     ]
    }
   ],
   "source": [
    "run('flickr30k', 'en')  # not used at all\n",
    "run('flickr30k', 'de')\n",
    "run('flickr30k', 'fr')\n",
    "run('flickr30k', 'cs')  # not used at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### load json path from ./annotations/msrvtt/en/train.json\n",
      "### there are 6513 unique images/videos, 130260 vision-caption pairs\n",
      "0.01 is not applicible\n",
      "--- generating a training subset of 0.1% (6) unique images/videos\n",
      "./annotations/msrvtt/en/subsets/0.1%_0.json\n",
      "./annotations/msrvtt/en/subsets/0.1%_1.json\n",
      "./annotations/msrvtt/en/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (65) unique images/videos\n",
      "./annotations/msrvtt/en/subsets/1%_0.json\n",
      "./annotations/msrvtt/en/subsets/1%_1.json\n",
      "./annotations/msrvtt/en/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (651) unique images/videos\n",
      "./annotations/msrvtt/en/subsets/10%_0.json\n",
      "./annotations/msrvtt/en/subsets/10%_1.json\n",
      "./annotations/msrvtt/en/subsets/10%_2.json\n"
     ]
    }
   ],
   "source": [
    "run('msrvtt', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### load json path from ./annotations/vatex/en/train.json\n",
      "### there are 25006 unique images/videos, 250060 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/vatex/en/subsets/0.01%_0.json\n",
      "./annotations/vatex/en/subsets/0.01%_1.json\n",
      "./annotations/vatex/en/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (25) unique images/videos\n",
      "./annotations/vatex/en/subsets/0.1%_0.json\n",
      "./annotations/vatex/en/subsets/0.1%_1.json\n",
      "./annotations/vatex/en/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (250) unique images/videos\n",
      "./annotations/vatex/en/subsets/1%_0.json\n",
      "./annotations/vatex/en/subsets/1%_1.json\n",
      "./annotations/vatex/en/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2500) unique images/videos\n",
      "./annotations/vatex/en/subsets/10%_0.json\n",
      "./annotations/vatex/en/subsets/10%_1.json\n",
      "./annotations/vatex/en/subsets/10%_2.json\n",
      "### load json path from ./annotations/vatex/zh/train.json\n",
      "### there are 25006 unique images/videos, 250060 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/vatex/zh/subsets/0.01%_0.json\n",
      "./annotations/vatex/zh/subsets/0.01%_1.json\n",
      "./annotations/vatex/zh/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (25) unique images/videos\n",
      "./annotations/vatex/zh/subsets/0.1%_0.json\n",
      "./annotations/vatex/zh/subsets/0.1%_1.json\n",
      "./annotations/vatex/zh/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (250) unique images/videos\n",
      "./annotations/vatex/zh/subsets/1%_0.json\n",
      "./annotations/vatex/zh/subsets/1%_1.json\n",
      "./annotations/vatex/zh/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2500) unique images/videos\n",
      "./annotations/vatex/zh/subsets/10%_0.json\n",
      "./annotations/vatex/zh/subsets/10%_1.json\n",
      "./annotations/vatex/zh/subsets/10%_2.json\n"
     ]
    }
   ],
   "source": [
    "run('vatex', 'en')  # not used at all\n",
    "run('vatex', 'zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e009a682218d944d4ca170df67ae304433d3da7f548affd1bafd64b5b995fea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
