{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SAVE_PATH = './annotations'\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Flickr30k\n",
    "\n",
    "```bibtex\n",
    "@article{young2014image,\n",
    "  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},\n",
    "  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},\n",
    "  journal={Transactions of the Association for Computational Linguistics},\n",
    "  volume={2},\n",
    "  pages={67--78},\n",
    "  year={2014},\n",
    "  publisher={MIT Press}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'flickr30k'\n",
    "RELATIVE_PATH_FORMAT = 'flickr30k-images/{}.jpg'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Flickr30k-EN (Karpathy's splits)\n",
    "```bibtex\n",
    "@inproceedings{karpathy2015deep,\n",
    "  title={Deep visual-semantic alignments for generating image descriptions},\n",
    "  author={Karpathy, Andrej and Fei-Fei, Li},\n",
    "  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n",
    "  pages={3128--3137},\n",
    "  year={2015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-14 21:00:07--  https://cs.stanford.edu/people/karpathy/deepimagesent/flickr30k.zip\n",
      "正在解析主机 cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
      "正在连接 cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... 已连接。\n",
      "警告: 无法验证 cs.stanford.edu 的由 “CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US” 颁发的证书:\n",
      "  出现了自己签名的证书。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：197966511 (189M) [application/zip]\n",
      "正在保存至: “flickr30k.zip”\n",
      "\n",
      "flickr30k.zip       100%[===================>] 188.79M  5.49MB/s  用时 36s       \n",
      "\n",
      "2023-02-14 21:00:43 (5.30 MB/s) - 已保存 “flickr30k.zip” [197966511/197966511])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cs.stanford.edu/people/karpathy/deepimagesent/flickr30k.zip --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  flickr30k.zip\n",
      "  inflating: flickr30k/dataset.json  \n",
      "  inflating: flickr30k/vgg_feats.mat  \n",
      "  inflating: flickr30k/readme.txt    \n"
     ]
    }
   ],
   "source": [
    "!unzip -o flickr30k.zip\n",
    "!rm flickr30k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('flickr30k/dataset.json', 'r'))['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentids': [0, 1, 2, 3, 4],\n",
       " 'imgid': 0,\n",
       " 'sentences': [{'tokens': ['two',\n",
       "    'young',\n",
       "    'guys',\n",
       "    'with',\n",
       "    'shaggy',\n",
       "    'hair',\n",
       "    'look',\n",
       "    'at',\n",
       "    'their',\n",
       "    'hands',\n",
       "    'while',\n",
       "    'hanging',\n",
       "    'out',\n",
       "    'in',\n",
       "    'the',\n",
       "    'yard'],\n",
       "   'raw': 'Two young guys with shaggy hair look at their hands while hanging out in the yard.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 0},\n",
       "  {'tokens': ['two',\n",
       "    'young',\n",
       "    'white',\n",
       "    'males',\n",
       "    'are',\n",
       "    'outside',\n",
       "    'near',\n",
       "    'many',\n",
       "    'bushes'],\n",
       "   'raw': 'Two young, White males are outside near many bushes.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 1},\n",
       "  {'tokens': ['two',\n",
       "    'men',\n",
       "    'in',\n",
       "    'green',\n",
       "    'shirts',\n",
       "    'are',\n",
       "    'standing',\n",
       "    'in',\n",
       "    'a',\n",
       "    'yard'],\n",
       "   'raw': 'Two men in green shirts are standing in a yard.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 2},\n",
       "  {'tokens': ['a',\n",
       "    'man',\n",
       "    'in',\n",
       "    'a',\n",
       "    'blue',\n",
       "    'shirt',\n",
       "    'standing',\n",
       "    'in',\n",
       "    'a',\n",
       "    'garden'],\n",
       "   'raw': 'A man in a blue shirt standing in a garden.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 3},\n",
       "  {'tokens': ['two', 'friends', 'enjoy', 'time', 'spent', 'together'],\n",
       "   'raw': 'Two friends enjoy time spent together.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 4}],\n",
       " 'split': 'train',\n",
       " 'filename': '1000092795.jpg'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "imageid2captions = defaultdict(list)\n",
    "imageid2split = {}\n",
    "split2imageids = defaultdict(list)\n",
    "for item in data:\n",
    "    imageid = int(item['filename'].split('.')[0])\n",
    "    for sentence in item['sentences']:\n",
    "        caption = sentence['raw']\n",
    "        imageid2captions[imageid].append(caption)\n",
    "    imageid2split[imageid] = item['split']\n",
    "    split2imageids[item['split']].append(imageid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test', 'train', 'val'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list(imageid2split.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'en')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "caption_id = 0\n",
    "\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    json_data = []\n",
    "    for image_id in split2imageids[mode]:\n",
    "        if mode == 'train':\n",
    "            for caption in imageid2captions[image_id]:\n",
    "                item = dict(\n",
    "                    image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                    caption=caption,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "        else:\n",
    "            item = dict(\n",
    "                image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                caption=imageid2captions[image_id],\n",
    "                image_id=image_id,\n",
    "            )\n",
    "            json_data.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(json_data, wf)\n",
    "\n",
    "    if mode != 'train':\n",
    "        gt = {\n",
    "            'annotations': [],\n",
    "            'images': [],\n",
    "        }\n",
    "\n",
    "        for item in json_data:\n",
    "            assert isinstance(item['caption'], (list, tuple))\n",
    "            image_id = item['image_id']\n",
    "            for caption in item['caption']:\n",
    "                item = dict(\n",
    "                    image_id=image_id,\n",
    "                    caption=caption,\n",
    "                    id=caption_id,\n",
    "                )\n",
    "                caption_id += 1\n",
    "                gt['annotations'].append(item)\n",
    "            gt['images'].append({'id': image_id})\n",
    "                    \n",
    "        with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "            json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Flickr30k-ZH\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{lan2017fluency,\n",
    "  title={Fluency-guided cross-lingual image captioning},\n",
    "  author={Lan, Weiyu and Li, Xirong and Dong, Jianfeng},\n",
    "  booktitle={Proceedings of the 25th ACM international conference on Multimedia},\n",
    "  pages={1549--1557},\n",
    "  year={2017}\n",
    "}\n",
    "```\n",
    "\n",
    "**Note**: We found that Flickr30k-CN does not follow the splits proposed by Karpathy et al. (https://cs.stanford.edu/people/karpathy/deepimagesent/flickr30k.zip). Besides, some images required by Flicrk30k-CN does not exist in Karpathy's splits (31,014 images) but exist in the official splits (31783 images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**please manually operates as follows:**\n",
    "1. Download Flickr30k-CN from https://github.com/li-xirong/cross-lingual-cap;\n",
    "2. Get the file named `seg.flickr30kzhmbosontest.caption.txt`;\n",
    "3. Put the file to the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "zh_imageid2captions = defaultdict(list)\n",
    "zh_data = open('seg.flickr30kzhmbosontest.caption.txt', 'r').read().strip().split('\\n')\n",
    "for line in zh_data:\n",
    "    splits = line.split(' ')\n",
    "    \n",
    "    tag = splits[0]\n",
    "    imageid = int(tag.split('#')[0]) # e.g., 1009692167#zhm#1 -> 1009692167\n",
    "\n",
    "    tokens = [item.split(':')[0] for item in splits[1:]] # e.g., 一个:m -> 一个\n",
    "    caption = ''.join(tokens)\n",
    "\n",
    "    zh_imageid2captions[imageid].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'zh')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "json_data = []\n",
    "for image_id, captions in zh_imageid2captions.items():\n",
    "    item = dict(\n",
    "        image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "        caption=captions,\n",
    "        image_id=image_id,\n",
    "    )\n",
    "    json_data.append(item)\n",
    "\n",
    "with open(os.path.join(save_path, 'test.json'), 'w') as wf:\n",
    "    json.dump(json_data, wf)\n",
    "\n",
    "gt = {\n",
    "    'annotations': [],\n",
    "    'images': [],\n",
    "}\n",
    "\n",
    "caption_id = 0\n",
    "for item in json_data:\n",
    "    assert isinstance(item['caption'], (list, tuple))\n",
    "    image_id = item['image_id']\n",
    "    for caption in item['caption']:\n",
    "        item = dict(\n",
    "            image_id=image_id,\n",
    "            caption=caption,\n",
    "            id=caption_id,\n",
    "        )\n",
    "        caption_id += 1\n",
    "        gt['annotations'].append(item)\n",
    "    gt['images'].append({'id': image_id})\n",
    "            \n",
    "with open(os.path.join(save_path, f'test_gt.json'), 'w') as wf:\n",
    "    json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Flickr30k-DE, Fr, Cs (Multi30K)\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{elliott2016multi30k,\n",
    "  title={Multi30K: Multilingual English-German Image Descriptions},\n",
    "  author={Elliott, Desmond and Frank, Stella and Sima’an, Khalil and Specia, Lucia},\n",
    "  booktitle={Proceedings of the 5th Workshop on Vision and Language},\n",
    "  pages={70--74},\n",
    "  year={2016}\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dataset'...\n",
      "remote: Enumerating objects: 313, done.\u001b[K\n",
      "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
      "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
      "remote: Total 313 (delta 17), reused 21 (delta 16), pack-reused 281\u001b[K\n",
      "Receiving objects: 100% (313/313), 18.21 MiB | 16.00 KiB/s, done.\n",
      "Resolving deltas: 100% (69/69), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/multi30k/dataset.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.1 Flickr30k-DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import gzip\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'de')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['train', 'val', 'test_2016']:\n",
    "    images_list = open(f'dataset/data/task2/image_splits/{mode}_images.txt', 'r').read().strip().split('\\n')\n",
    "    captions_list = []\n",
    "    for i in range(5):\n",
    "        data = []\n",
    "        with gzip.open(f'dataset/data/task2/raw/{mode}.{i+1}.de.gz', 'rt', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                caption = line.strip()\n",
    "                data.append(caption)\n",
    "        assert len(data) == len(images_list)\n",
    "        captions_list.append(data)\n",
    "\n",
    "    mode = mode.split('_')[0] # 'test_2016' -> 'test'\n",
    "    \n",
    "    imageid2captions = {}\n",
    "    for fn, captions in zip(images_list, zip(*captions_list)):\n",
    "        image_id = int(fn.split('.')[0])\n",
    "        imageid2captions[image_id] = list(captions)\n",
    "\n",
    "    json_data = []\n",
    "    for image_id, captions in imageid2captions.items():\n",
    "        if mode == 'train':\n",
    "            for caption in captions:\n",
    "                item = dict(\n",
    "                    image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                    caption=caption,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "        else:\n",
    "            item = dict(\n",
    "                image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                caption=captions,\n",
    "                image_id=image_id,\n",
    "            )\n",
    "            json_data.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(json_data, wf)\n",
    "\n",
    "    if mode != 'train':\n",
    "        gt = {\n",
    "            'annotations': [],\n",
    "            'images': [],\n",
    "        }\n",
    "\n",
    "        for item in json_data:\n",
    "            assert isinstance(item['caption'], (list, tuple))\n",
    "            image_id = item['image_id']\n",
    "            for caption in item['caption']:\n",
    "                item = dict(\n",
    "                    image_id=image_id,\n",
    "                    caption=caption,\n",
    "                    id=caption_id,\n",
    "                )\n",
    "                caption_id += 1\n",
    "                gt['annotations'].append(item)\n",
    "            gt['images'].append({'id': image_id})\n",
    "                    \n",
    "        with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "            json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3.2 Flickr30k-FR, CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_special_tokens(data):\n",
    "    new_data = []\n",
    "    for line in data:\n",
    "        # in translation data, `man 's` is wrriten as `man &apos;s`\n",
    "        line = line.replace(\"&apos;\", \"'\")\n",
    "        # in translation data, `\" 92 \"` is wrriten as `&quot; 92 &quot;`\n",
    "        line = line.replace('&quot;', '\"')\n",
    "        new_data.append(line)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import gzip\n",
    "\n",
    "for lang in ['fr', 'cs']:\n",
    "    save_path = os.path.join(SAVE_PATH, DATASET, lang)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    caption_id = 0\n",
    "    for mode in ['train', 'val', 'test_2016_flickr']:\n",
    "        images_list = open(f'dataset/data/task1/image_splits/{mode}.txt', 'r').read().strip().split('\\n')\n",
    "        captions_list = []\n",
    "        for i in range(1):\n",
    "            data = []\n",
    "            with gzip.open(f'dataset/data/task1/raw/{mode}.{lang}.gz', 'rt', encoding='utf8') as f:\n",
    "                for line in f:\n",
    "                    caption = line.strip()\n",
    "                    data.append(caption)\n",
    "\n",
    "            assert len(data) == len(images_list)\n",
    "            data = replace_special_tokens(data)\n",
    "            captions_list.append(data)\n",
    "        \n",
    "        mode = mode.split('_')[0] # 'test_2016_flickr' -> 'test'\n",
    "        \n",
    "        imageid2captions = {}\n",
    "        for fn, captions in zip(images_list, zip(*captions_list)):\n",
    "            image_id = int(fn.split('.')[0])\n",
    "            imageid2captions[image_id] = list(captions)\n",
    "\n",
    "        json_data = []\n",
    "        for image_id, captions in imageid2captions.items():\n",
    "            if mode == 'train':\n",
    "                for caption in captions:\n",
    "                    item = dict(\n",
    "                        image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                        caption=caption,\n",
    "                        image_id=image_id,\n",
    "                    )\n",
    "                    json_data.append(item)\n",
    "            else:\n",
    "                item = dict(\n",
    "                    image=RELATIVE_PATH_FORMAT.format(image_id),\n",
    "                    caption=captions,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "\n",
    "        with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "            json.dump(json_data, wf)\n",
    "\n",
    "        if mode != 'train':\n",
    "            gt = {\n",
    "                'annotations': [],\n",
    "                'images': [],\n",
    "            }\n",
    "\n",
    "            for item in json_data:\n",
    "                assert isinstance(item['caption'], (list, tuple))\n",
    "                image_id = item['image_id']\n",
    "                for caption in item['caption']:\n",
    "                    item = dict(\n",
    "                        image_id=image_id,\n",
    "                        caption=caption,\n",
    "                        id=caption_id,\n",
    "                    )\n",
    "                    caption_id += 1\n",
    "                    gt['annotations'].append(item)\n",
    "                gt['images'].append({'id': image_id})\n",
    "                        \n",
    "            with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "                json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.1 Flickr30k EN-ZH Pairs\n",
    "\n",
    "**Note**: please run `Section 1.2 Flicrk30k-ZH` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-23 01:22:34--  http://shannon.cs.illinois.edu/DenotationGraph/data/flickr30k.tar.gz\n",
      "正在解析主机 shannon.cs.illinois.edu (shannon.cs.illinois.edu)... 18.220.149.166\n",
      "正在连接 shannon.cs.illinois.edu (shannon.cs.illinois.edu)|18.220.149.166|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：3652513 (3.5M) [application/x-gzip]\n",
      "正在保存至: “flickr30k.tar.gz”\n",
      "\n",
      "flickr30k.tar.gz    100%[===================>]   3.48M  46.7KB/s  用时 53s       \n",
      "\n",
      "2023-03-23 01:23:28 (67.5 KB/s) - 已保存 “flickr30k.tar.gz” [3652513/3652513])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the official flickr30k annotations and get the file named results_20130124.token\n",
    "!wget http://shannon.cs.illinois.edu/DenotationGraph/data/flickr30k.tar.gz --no-check-certificate\n",
    "!tar -xzf flickr30k.tar.gz\n",
    "!rm flickr30k.tar.gz\n",
    "!rm readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "en_imageid2captions = defaultdict(list)\n",
    "official_data = open('results_20130124.token', 'r').read().strip().split('\\n')\n",
    "for line in official_data:\n",
    "    tag, caption = line.split('\\t')\n",
    "    imageid = int(tag.split('.')[0]) # e.g., 1000092795.jpg#0 -> 1000092795\n",
    "    en_imageid2captions[imageid].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1009692167,\n",
       " 1021439420,\n",
       " 1032122270,\n",
       " 1043819504,\n",
       " 1095580424,\n",
       " 11034843,\n",
       " 11214470,\n",
       " 1128230658,\n",
       " 1132772170,\n",
       " 1143882946]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_imageids = list(zh_imageid2captions.keys())\n",
    "zh_imageids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在警车前，一条训练有素的警犬坐在它的警官身旁。',\n",
       " '一名警察站着，身边有一只德国牧羊犬',\n",
       " '一位安保人员带着他的狗正在寻找某些东西',\n",
       " '一名穿着反光背心的军官和他的狗站在他的车前面',\n",
       " '一个警察和一只搜索犬在街上']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_imageid2captions[zh_imageids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An officer in a reflective vest stands at the front of his van with his dog .',\n",
       " 'A trained police dog sits next to his handler in front of the police van .',\n",
       " 'A security man with his watch dog is looking for something .',\n",
       " 'A policeman is standing with a German Shepherd dog .',\n",
       " 'A policeman stops on a street with a search dog .']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_imageid2captions[zh_imageids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(SAVE_PATH, DATASET, 'en-zh')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "en_captions = [en_imageid2captions[image_id][i] for image_id in zh_imageids for i in range(5)]\n",
    "zh_captions = [zh_imageid2captions[image_id][i] for image_id in zh_imageids for i in range(5)]\n",
    "\n",
    "# add a full stop at the end of each sentence\n",
    "for i in range(len(en_captions)):\n",
    "    en_captions[i] = (en_captions[i] + ' .') if not en_captions[i].endswith(' .') else en_captions[i]\n",
    "    zh_captions[i] = (zh_captions[i] + '。') if not zh_captions[i].endswith('。') else zh_captions[i]\n",
    "\n",
    "with open(os.path.join(path, 'test.en'), 'w') as wf:\n",
    "    wf.write('\\n'.join(en_captions))\n",
    "\n",
    "with open(os.path.join(path, 'test.zh'), 'w') as wf:\n",
    "    wf.write('\\n'.join(zh_captions))\n",
    "\n",
    "image_rpaths = [RELATIVE_PATH_FORMAT.format(image_id) for image_id in zh_imageids for _ in range(5)]\n",
    "with open(os.path.join(path, 'test_images.txt'), 'w') as wf:\n",
    "    wf.write('\\n'.join(image_rpaths))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.2 Flickr30k EN-DE, EN-FR, DE-FR Pairs\n",
    "\n",
    "**Note**: please run `Section 1.3 Flicrk30k-DE, FR,CS` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for folder in ['en-de', 'en-fr', 'de-fr']:\n",
    "    path = os.path.join(SAVE_PATH, DATASET, folder)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    for mode in ['train', 'val', 'test_2016_flickr']:\n",
    "        for lang in folder.split('-'):\n",
    "            src = f'dataset/data/task1/tok/{mode}.lc.norm.tok.{lang}'\n",
    "            data = open(src, 'r').read().strip().split('\\n')\n",
    "            data = replace_special_tokens(data)\n",
    "            \n",
    "            trg = os.path.join(path, f'{mode.split(\"_\")[0]}.{lang}')\n",
    "            with open(trg, 'w') as wf:\n",
    "                wf.write('\\n'.join(data))\n",
    "        \n",
    "        image_ids = [item.split('.')[0] for item in open(f'dataset/data/task1/image_splits/{mode}.txt', 'r').read().strip().split('\\n')]\n",
    "        image_rpaths = [RELATIVE_PATH_FORMAT.format(image_id) for image_id in image_ids]\n",
    "\n",
    "        with open(os.path.join(path, f'{mode.split(\"_\")[0]}_images.txt'), 'w') as wf:\n",
    "            wf.write('\\n'.join(image_rpaths))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.3 Flickr30k ZH-DE, ZH-FR Pairs\n",
    "\n",
    "**Note**: please run `Section 1.2 Flicrk30k-ZH` and `Section 1.3 Flicrk30k-DE, FR,CS` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "zh_imageids = set([item['image_id'] for item in json.load(open(os.path.join(SAVE_PATH, DATASET, 'zh', 'test.json'), 'r'))])\n",
    "de_fr_imageids = set([item['image_id'] for item in json.load(open(os.path.join(SAVE_PATH, DATASET, 'de', 'test.json'), 'r'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(zh_imageids), len(de_fr_imageids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zh_imageids & de_fr_imageids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_zh_path = os.path.join(SAVE_PATH, DATASET, 'en-zh')\n",
    "en_captions = open(os.path.join(en_zh_path, 'test.en'), 'r').read().strip().split('\\n')\n",
    "zh_captions = open(os.path.join(en_zh_path, 'test.zh'), 'r').read().strip().split('\\n')\n",
    "en2zh = {item[0].lower(): item[1] for item in zip(*[en_captions, zh_captions])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_captions = open(os.path.join(SAVE_PATH, DATASET, 'en-de', 'test.en'), 'r').read().strip().split('\\n')\n",
    "\n",
    "for i in range(len(en_captions)):\n",
    "    # add a full stop to each sentence of en_captions\n",
    "    en_captions[i] = (en_captions[i] + ' .') if not en_captions[i].endswith(' .') else en_captions[i]\n",
    "\n",
    "de_captions = open(os.path.join(SAVE_PATH, DATASET, 'en-de', 'test.de'), 'r').read().strip().split('\\n')\n",
    "fr_captions = open(os.path.join(SAVE_PATH, DATASET, 'en-fr', 'test.fr'), 'r').read().strip().split('\\n')\n",
    "\n",
    "en2de = {item[0].lower(): item[1] for item in zip(*[en_captions, de_captions])}\n",
    "en2fr = {item[0].lower(): item[1] for item in zip(*[en_captions, fr_captions])}\n",
    "\n",
    "imageids = open(os.path.join(SAVE_PATH, DATASET, 'en-de', 'test_images.txt'), 'r').read().strip().split('\\n')\n",
    "en2imageid = {item[0].lower(): item[1].split('/')[-1].split('.')[0] for item in zip(*[en_captions, imageids])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_en = set(list(en2zh.keys())) & set(list(en2de.keys()))\n",
    "len(common_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trg_lang, trg_map in zip(['de', 'fr'], [en2de, en2fr]):\n",
    "    path = os.path.join(SAVE_PATH, DATASET, f'zh-{trg_lang}')\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(path, 'test.zh'), 'w') as wf:\n",
    "        wf.write('\\n'.join([en2zh[en] for en in common_en]))\n",
    "    \n",
    "    with open(os.path.join(path, f'test.{trg_lang}'), 'w') as wf:\n",
    "        wf.write('\\n'.join([trg_map[en] for en in common_en]))\n",
    "    \n",
    "    image_rpaths = [RELATIVE_PATH_FORMAT.format(en2imageid[en]) for en in common_en]\n",
    "    with open(os.path.join(path, 'test_images.txt'), 'w') as wf:\n",
    "        wf.write('\\n'.join(image_rpaths))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MS-COCO\n",
    "\n",
    "```bibtex\n",
    "@article{chen2015microsoft,\n",
    "  title={Microsoft coco captions: Data collection and evaluation server},\n",
    "  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\\'a}r, Piotr and Zitnick, C Lawrence},\n",
    "  journal={arXiv preprint arXiv:1504.00325},\n",
    "  year={2015}\n",
    "}\n",
    "@inproceedings{lin2014microsoft,\n",
    "  title={Microsoft coco: Common objects in context},\n",
    "  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\\'a}r, Piotr and Zitnick, C Lawrence},\n",
    "  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},\n",
    "  pages={740--755},\n",
    "  year={2014},\n",
    "  organization={Springer}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'coco'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 COCO-EN (Karpathy's splits)\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{karpathy2015deep,\n",
    "  title={Deep visual-semantic alignments for generating image descriptions},\n",
    "  author={Karpathy, Andrej and Fei-Fei, Li},\n",
    "  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n",
    "  pages={3128--3137},\n",
    "  year={2015}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-23 14:19:46--  https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
      "正在解析主机 cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
      "正在连接 cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... 已连接。\n",
      "警告: 无法验证 cs.stanford.edu 的由 “CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US” 颁发的证书:\n",
      "  出现了自己签名的证书。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：36745453 (35M) [application/zip]\n",
      "正在保存至: “caption_datasets.zip”\n",
      "\n",
      "caption_datasets.zi 100%[===================>]  35.04M  4.20MB/s  用时 19s       \n",
      "\n",
      "2023-03-23 14:20:06 (1.83 MB/s) - 已保存 “caption_datasets.zip” [36745453/36745453])\n",
      "\n",
      "Archive:  caption_datasets.zip\n",
      "  inflating: dataset_coco.json       \n",
      "  inflating: dataset_flickr30k.json  \n",
      "  inflating: dataset_flickr8k.json   \n"
     ]
    }
   ],
   "source": [
    "!wget https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip --no-check-certificate\n",
    "!unzip caption_datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('dataset_coco.json', 'r'))['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': 'val2014',\n",
       " 'sentids': [770337, 771687, 772707, 776154, 781998],\n",
       " 'filename': 'COCO_val2014_000000391895.jpg',\n",
       " 'imgid': 0,\n",
       " 'split': 'test',\n",
       " 'sentences': [{'tokens': ['a',\n",
       "    'man',\n",
       "    'with',\n",
       "    'a',\n",
       "    'red',\n",
       "    'helmet',\n",
       "    'on',\n",
       "    'a',\n",
       "    'small',\n",
       "    'moped',\n",
       "    'on',\n",
       "    'a',\n",
       "    'dirt',\n",
       "    'road'],\n",
       "   'raw': 'A man with a red helmet on a small moped on a dirt road. ',\n",
       "   'imgid': 0,\n",
       "   'sentid': 770337},\n",
       "  {'tokens': ['man',\n",
       "    'riding',\n",
       "    'a',\n",
       "    'motor',\n",
       "    'bike',\n",
       "    'on',\n",
       "    'a',\n",
       "    'dirt',\n",
       "    'road',\n",
       "    'on',\n",
       "    'the',\n",
       "    'countryside'],\n",
       "   'raw': 'Man riding a motor bike on a dirt road on the countryside.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 771687},\n",
       "  {'tokens': ['a',\n",
       "    'man',\n",
       "    'riding',\n",
       "    'on',\n",
       "    'the',\n",
       "    'back',\n",
       "    'of',\n",
       "    'a',\n",
       "    'motorcycle'],\n",
       "   'raw': 'A man riding on the back of a motorcycle.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 772707},\n",
       "  {'tokens': ['a',\n",
       "    'dirt',\n",
       "    'path',\n",
       "    'with',\n",
       "    'a',\n",
       "    'young',\n",
       "    'person',\n",
       "    'on',\n",
       "    'a',\n",
       "    'motor',\n",
       "    'bike',\n",
       "    'rests',\n",
       "    'to',\n",
       "    'the',\n",
       "    'foreground',\n",
       "    'of',\n",
       "    'a',\n",
       "    'verdant',\n",
       "    'area',\n",
       "    'with',\n",
       "    'a',\n",
       "    'bridge',\n",
       "    'and',\n",
       "    'a',\n",
       "    'background',\n",
       "    'of',\n",
       "    'cloud',\n",
       "    'wreathed',\n",
       "    'mountains'],\n",
       "   'raw': 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ',\n",
       "   'imgid': 0,\n",
       "   'sentid': 776154},\n",
       "  {'tokens': ['a',\n",
       "    'man',\n",
       "    'in',\n",
       "    'a',\n",
       "    'red',\n",
       "    'shirt',\n",
       "    'and',\n",
       "    'a',\n",
       "    'red',\n",
       "    'hat',\n",
       "    'is',\n",
       "    'on',\n",
       "    'a',\n",
       "    'motorcycle',\n",
       "    'on',\n",
       "    'a',\n",
       "    'hill',\n",
       "    'side'],\n",
       "   'raw': 'A man in a red shirt and a red hat is on a motorcycle on a hill side.',\n",
       "   'imgid': 0,\n",
       "   'sentid': 781998}],\n",
       " 'cocoid': 391895}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "imageid2rpath = {}\n",
    "imageid2captions = defaultdict(list)\n",
    "split2imageids = defaultdict(list)\n",
    "for item in data:\n",
    "    imageid = int(item['filename'].split('.')[0].split('_')[-1])\n",
    "    for sentence in item['sentences']:\n",
    "        caption = sentence['raw']\n",
    "        imageid2captions[imageid].append(caption)\n",
    "    split2imageids[item['split']].append(imageid)\n",
    "    imageid2rpath[imageid] =  os.path.join(item['filepath'], item['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 5000\n",
      "restval 30504\n",
      "val 5000\n",
      "train 82783\n"
     ]
    }
   ],
   "source": [
    "for k, v in split2imageids.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'en')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    json_data = []\n",
    "\n",
    "    if mode == 'train':\n",
    "        image_ids_of_this_split = split2imageids['restval'] + split2imageids[mode]\n",
    "    else:\n",
    "        image_ids_of_this_split = split2imageids[mode]\n",
    "\n",
    "    for image_id in image_ids_of_this_split:\n",
    "        if mode == 'train':\n",
    "            for caption in imageid2captions[image_id]:\n",
    "                item = dict(\n",
    "                    image=imageid2rpath[image_id],\n",
    "                    caption=caption,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "        else:\n",
    "            item = dict(\n",
    "                image=imageid2rpath[image_id],\n",
    "                caption=imageid2captions[image_id],\n",
    "                image_id=image_id,\n",
    "            )\n",
    "            json_data.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(json_data, wf)\n",
    "\n",
    "    if mode != 'train':\n",
    "        gt = {\n",
    "            'annotations': [],\n",
    "            'images': [],\n",
    "        }\n",
    "\n",
    "        for item in json_data:\n",
    "            assert isinstance(item['caption'], (list, tuple))\n",
    "            image_id = item['image_id']\n",
    "            for caption in item['caption']:\n",
    "                item = dict(\n",
    "                    image_id=image_id,\n",
    "                    caption=caption,\n",
    "                    id=caption_id,\n",
    "                )\n",
    "                caption_id += 1\n",
    "                gt['annotations'].append(item)\n",
    "            gt['images'].append({'id': image_id})\n",
    "                    \n",
    "        with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "            json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 COCO-JA\n",
    "\n",
    "```bibtex\n",
    "@InProceedings{Yoshikawa2017,\n",
    "  title     = {STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset},\n",
    "  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},\n",
    "  month     = {July},\n",
    "  year      = {2017},\n",
    "  address   = {Vancouver, Canada},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {417--421},\n",
    "  url       = {http://www.aclweb.org/anthology/P17-2066}\n",
    "}\n",
    "```\n",
    "\n",
    "Please run `Section 2.1 COCO-EN` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-23 15:06:17--  https://github.com/STAIR-Lab-CIT/STAIR-captions/raw/master/stair_captions_v1.2.tar.gz\n",
      "正在解析主机 github.com (github.com)... 140.82.113.4\n",
      "正在连接 github.com (github.com)|140.82.113.4|:443... 已连接。\n",
      "警告: 无法验证 github.com 的由 “CN=DigiCert TLS Hybrid ECC SHA384 2020 CA1,O=DigiCert Inc,C=US” 颁发的证书:\n",
      "  无法本地校验颁发者的权限。\n",
      "已发出 HTTP 请求，正在等待回应... 302 Found\n",
      "位置：https://raw.githubusercontent.com/STAIR-Lab-CIT/STAIR-captions/master/stair_captions_v1.2.tar.gz [跟随至新的 URL]\n",
      "--2023-03-23 15:06:18--  https://raw.githubusercontent.com/STAIR-Lab-CIT/STAIR-captions/master/stair_captions_v1.2.tar.gz\n",
      "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.96.133\n",
      "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|199.232.96.133|:443... ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/STAIR-Lab-CIT/STAIR-captions/raw/master/stair_captions_v1.2.tar.gz --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x stair_captions_v1.2_train.json\n",
      "x stair_captions_v1.2_train_tokenized.json\n",
      "x stair_captions_v1.2_val.json\n",
      "x stair_captions_v1.2_val_tokenized.json\n"
     ]
    }
   ],
   "source": [
    "!tar -xzvf stair_captions_v1.2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'images', 'licenses', 'annotations'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "train_data = json.load(open('stair_captions_v1.2_train.json', 'r'))\n",
    "val_data = json.load(open('stair_captions_v1.2_val.json', 'r'))\n",
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 203312, 'id': 3, 'caption': '山の中を赤い電車が走っている'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'license': 5,\n",
       " 'file_name': 'COCO_train2014_000000057870.jpg',\n",
       " 'coco_url': 'http://mscoco.org/images/57870',\n",
       " 'height': 480,\n",
       " 'width': 640,\n",
       " 'date_captured': '2013-11-14 16:28:13',\n",
       " 'flickr_url': 'http://farm4.staticflickr.com/3153/2970773875_164f0c0b83_z.jpg',\n",
       " 'id': 57870}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "imageid2captions = defaultdict(list)\n",
    "for item in train_data['annotations'] + val_data['annotations']:\n",
    "    imageid2captions[item['image_id']].append(item['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'ja')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    json_data = []\n",
    "    # split2imageids is obtained by Section 2.1, so COCO-JA shares the same Karpathy's splits\n",
    "    if mode == 'train':\n",
    "        image_ids_of_this_split = split2imageids['restval'] + split2imageids[mode]\n",
    "    else:\n",
    "        image_ids_of_this_split = split2imageids[mode]\n",
    "\n",
    "    for image_id in image_ids_of_this_split:\n",
    "        if mode == 'train':\n",
    "            for caption in imageid2captions[image_id]:\n",
    "                item = dict(\n",
    "                    image=imageid2rpath[image_id],\n",
    "                    caption=caption,\n",
    "                    image_id=image_id,\n",
    "                )\n",
    "                json_data.append(item)\n",
    "        else:\n",
    "            item = dict(\n",
    "                image=imageid2rpath[image_id],\n",
    "                caption=imageid2captions[image_id],\n",
    "                image_id=image_id,\n",
    "            )\n",
    "            json_data.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(json_data, wf)\n",
    "\n",
    "    if mode != 'train':\n",
    "        gt = {\n",
    "            'annotations': [],\n",
    "            'images': [],\n",
    "        }\n",
    "\n",
    "        for item in json_data:\n",
    "            assert isinstance(item['caption'], (list, tuple))\n",
    "            image_id = item['image_id']\n",
    "            for caption in item['caption']:\n",
    "                item = dict(\n",
    "                    image_id=image_id,\n",
    "                    caption=caption,\n",
    "                    id=caption_id,\n",
    "                )\n",
    "                caption_id += 1\n",
    "                gt['annotations'].append(item)\n",
    "            gt['images'].append({'id': image_id})\n",
    "                    \n",
    "        with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "            json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MSR-VTT\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{xu2016msr,\n",
    "  title={Msr-vtt: A large video description dataset for bridging video and language},\n",
    "  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},\n",
    "  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\n",
    "  pages={5288--5296},\n",
    "  year={2016}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'msrvtt'\n",
    "RELATIVE_PATH_FORMAT = 'all_videos/video{vid}.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "json_path = os.path.join(save_path, 'videodatainfo_2016.json')\n",
    "\n",
    "if not os.path.exists(json_path):\n",
    "    # the official url is http://ms-multimedia-challenge.com/2016, but the website is out-of-date\n",
    "    url = \"https://github.com/ybCliff/VideoCaptioning/releases/download/v1.0/videodatainfo_2016.json\"\n",
    "    wget.download(url, json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'en')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "data = json.load(open(json_path, 'r'))\n",
    "splits = defaultdict(list)\n",
    "\n",
    "for item in data['videos']:\n",
    "    # 'video1000' -> 1000\n",
    "    vid = int(item['video_id'][5:])\n",
    "    splits[item['split']].append(vid)\n",
    "\n",
    "for k in splits.keys():\n",
    "    splits[k] = sorted(splits[k])\n",
    "\n",
    "splits['val'] = splits.pop('validate')\n",
    "\n",
    "vid2captions = defaultdict(list)\n",
    "\n",
    "for item in data['sentences']:\n",
    "    vid = int(item['video_id'][5:])\n",
    "    vid2captions[vid].append(item['caption'])\n",
    "\n",
    "train_items = []\n",
    "for vid in splits['train']:\n",
    "    for caption in vid2captions[vid]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid), # we only record the relative path of a video\n",
    "            caption=caption,\n",
    "            image_id=vid,\n",
    "        )\n",
    "        train_items.append(item)\n",
    "\n",
    "with open(os.path.join(save_path, 'train.json'), 'w') as wf:\n",
    "    json.dump(train_items, wf)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['val', 'test']:\n",
    "    items = []\n",
    "    for vid in splits[mode]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid),\n",
    "            caption=vid2captions[vid],\n",
    "            image_id=vid,\n",
    "        )\n",
    "        items.append(item)\n",
    "    \n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(items, wf)\n",
    "    \n",
    "    gt = {\n",
    "        'annotations': [],\n",
    "        'images': [{'id': vid} for vid in splits[mode]],\n",
    "    }\n",
    "\n",
    "    for vid in splits[mode]:\n",
    "        for caption in vid2captions[vid]:\n",
    "            item = dict(\n",
    "                image_id=vid,\n",
    "                caption=caption,\n",
    "                id=caption_id,\n",
    "            )\n",
    "            caption_id += 1\n",
    "            gt['annotations'].append(item)\n",
    "    \n",
    "    with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "        json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. VATEX\n",
    "```bibtex\n",
    "@inproceedings{wang2019vatex,\n",
    "  title={Vatex: A large-scale, high-quality multilingual dataset for video-and-language research},\n",
    "  author={Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},\n",
    "  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n",
    "  pages={4581--4591},\n",
    "  year={2019}\n",
    "}\n",
    "```\n",
    "\n",
    "**Notes: Some videos of this dataset can not be downloaded any more; We use the dataset identical to the following paper:**\n",
    "```bibtex\n",
    "@inproceedings{yang2022clip,\n",
    "  title={CLIP Meets Video Captioning: Concept-Aware Representation Learning Does Matter},\n",
    "  author={Yang, Bang and Zhang, Tong and Zou, Yuexian},\n",
    "  booktitle={Pattern Recognition and Computer Vision: 5th Chinese Conference, PRCV 2022, Shenzhen, China, November 4--7, 2022, Proceedings, Part I},\n",
    "  pages={368--381},\n",
    "  year={2022},\n",
    "}\n",
    "```\n",
    "url: https://github.com/yangbang18/CLIP-Captioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'vatex'\n",
    "RELATIVE_PATH_FORMAT = 'all_videos/video{vid}.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget \n",
    "\n",
    "root = os.path.join(SAVE_PATH, DATASET)\n",
    "os.makedirs(root, exist_ok=True)\n",
    "\n",
    "base_url = \"https://eric-xw.github.io/vatex-website/data/\"\n",
    "\n",
    "for filename in ['vatex_training_v1.0.json', 'vatex_validation_v1.0.json', 'vatex_public_test_english_v1.1.json']:\n",
    "    now_path = os.path.join(root, filename)\n",
    "    now_url = base_url + filename\n",
    "    if not os.path.exists(now_path):\n",
    "        wget.download(now_url, out=now_path)\n",
    "\n",
    "base_url = \"https://raw.githubusercontent.com/yangbang18/CLIP-Captioner/master/data/\"\n",
    "for filename in ['vatex_mapping.txt', 'vatex_existed_videos.txt']:\n",
    "    now_path = os.path.join(root, filename)\n",
    "    now_url = base_url + filename\n",
    "    if not os.path.exists(now_path):\n",
    "        wget.download(now_url, out=now_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 VATEX-EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `train` split has 25006 videos, 985 of 25991 videos are unaccessible!\n",
      "The `val` split has 2893 videos, 107 of 3000 videos are unaccessible!\n",
      "The `test` split has 5792 videos, 208 of 6000 videos are unaccessible!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'en')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "lines = open(os.path.join(root, 'vatex_mapping.txt'), 'r').read().strip().split('\\n')\n",
    "id2vid = {}\n",
    "for line in lines:\n",
    "    id, vid = line.split(' ')\n",
    "    id2vid[id] = vid # e.g., Ptf_2VRj-V0_000122_000132 -> video0\n",
    "\n",
    "existed_videos = open(os.path.join(root, 'vatex_existed_videos.txt'), 'r').read().strip().split('\\n')\n",
    "existed_videos = set([int(item[5:]) for item in existed_videos])\n",
    "\n",
    "splits = defaultdict(list)\n",
    "vid2captions = {}\n",
    "vid2captions_zh = {}\n",
    "for fn, split in zip(\n",
    "    ['vatex_training_v1.0.json', 'vatex_validation_v1.0.json', 'vatex_public_test_english_v1.1.json'], \n",
    "    ['train', 'val', 'test']):\n",
    "    data = json.load(open(os.path.join(root, fn), 'r'))\n",
    "\n",
    "    failed_count = 0\n",
    "    for item in data:\n",
    "        # Ptf_2VRj-V0_000122_000132 -> video0\n",
    "        vid = id2vid[item['videoID']]\n",
    "        # video0 -> 0\n",
    "        vid = int(vid[5:])\n",
    "        if vid not in existed_videos:\n",
    "            # we do not use the annotations of those unaccessible videos\n",
    "            failed_count += 1\n",
    "            continue\n",
    "\n",
    "        splits[split].append(vid)\n",
    "        vid2captions[vid] = item['enCap']\n",
    "        if split == 'test':\n",
    "            assert 'chCap' not in item\n",
    "        else:\n",
    "            vid2captions_zh[vid] = item['chCap']\n",
    "    \n",
    "    print(f'The `{split}` split has {len(data) - failed_count} videos, {failed_count} of {len(data)} videos are unaccessible!')\n",
    "\n",
    "for k in splits.keys():\n",
    "    splits[k] = sorted(splits[k])\n",
    "\n",
    "train_items = []\n",
    "for vid in splits['train']:\n",
    "    for caption in vid2captions[vid]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid), # we only record the relative path of a video\n",
    "            caption=caption,\n",
    "            image_id=vid,\n",
    "        )\n",
    "        train_items.append(item)\n",
    "\n",
    "with open(os.path.join(save_path, 'train.json'), 'w') as wf:\n",
    "    json.dump(train_items, wf)\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['val', 'test']:\n",
    "    items = []\n",
    "    for vid in splits[mode]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid),\n",
    "            caption=vid2captions[vid],\n",
    "            image_id=vid,\n",
    "        )\n",
    "        items.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(items, wf)\n",
    "    \n",
    "    gt = {\n",
    "        'annotations': [],\n",
    "        'images': [{'id': vid} for vid in splits[mode]],\n",
    "    }\n",
    "\n",
    "    for vid in splits[mode]:\n",
    "        for caption in vid2captions[vid]:\n",
    "            item = dict(\n",
    "                image_id=vid,\n",
    "                caption=caption,\n",
    "                id=caption_id,\n",
    "            )\n",
    "            caption_id += 1\n",
    "            gt['annotations'].append(item)\n",
    "    \n",
    "    with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "        json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 VATEX-ZH\n",
    "\n",
    "In the following, we provide the splits of VATEX-ZH identical to the paper:\n",
    "```bibtex\n",
    "@article{Yang2023ZeroNLG,\n",
    "   title={ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation},\n",
    "   author={Yang, Bang and Liu, Fenglin and Zou, Yuexian and Wu, Xian and Wang, Yaowei and Clifton, David A.},\n",
    "   journal={arXiv preprint arXiv:2303.06458}\n",
    "   year={2023}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `train` split has 25006 videos, 985 of 25991 videos are unaccessible!\n",
      "The `val` split has 2893 videos, 107 of 3000 videos are unaccessible!\n",
      "Constructing the test split by myself\n",
      "Now, train: val: test == 25006: 1393: 1500\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "save_path = os.path.join(SAVE_PATH, DATASET, 'zh')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "vid2captions_zh = {}\n",
    "for fn, split in zip(\n",
    "    ['vatex_training_v1.0.json', 'vatex_validation_v1.0.json', 'vatex_public_test_english_v1.1.json'], \n",
    "    ['train', 'val', 'test']):\n",
    "    data = json.load(open(os.path.join(root, fn), 'r'))\n",
    "\n",
    "    failed_count = 0\n",
    "    for item in data:\n",
    "        # Ptf_2VRj-V0_000122_000132 -> video0\n",
    "        vid = id2vid[item['videoID']]\n",
    "        # video0 -> 0\n",
    "        vid = int(vid[5:])\n",
    "        if vid not in existed_videos:\n",
    "            # we do not use the annotations of those unaccessible videos\n",
    "            failed_count += 1\n",
    "            continue\n",
    "\n",
    "        if split == 'test':\n",
    "            assert 'chCap' not in item\n",
    "        else:\n",
    "            vid2captions_zh[vid] = item['chCap']\n",
    "    \n",
    "    if split != 'test':\n",
    "        print(f'The `{split}` split has {len(data) - failed_count} videos, {failed_count} of {len(data)} videos are unaccessible!')\n",
    "\n",
    "train_items_zh = []\n",
    "for vid in splits['train']:\n",
    "    for caption in vid2captions_zh[vid]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid), # we only record the relative path of a video\n",
    "            caption=caption,\n",
    "            image_id=vid,\n",
    "        )\n",
    "        train_items_zh.append(item)\n",
    "\n",
    "with open(os.path.join(save_path, 'train.json'), 'w') as wf:\n",
    "    json.dump(train_items_zh, wf)\n",
    "\n",
    "\n",
    "print('Constructing the test split by myself')\n",
    "splits_zh = copy.deepcopy(splits)\n",
    "splits_zh['test'] = splits_zh['val'][-1500:]\n",
    "splits_zh['val'] = splits_zh['val'][:-1500]\n",
    "\n",
    "print(f'Now, train: val: test == {len(splits_zh[\"train\"])}: {len(splits_zh[\"val\"])}: {len(splits_zh[\"test\"])}')\n",
    "\n",
    "caption_id = 0\n",
    "for mode in ['val', 'test']:\n",
    "    items = []\n",
    "    for vid in splits_zh[mode]:\n",
    "        item = dict(\n",
    "            image=RELATIVE_PATH_FORMAT.format(vid=vid),\n",
    "            caption=vid2captions_zh[vid],\n",
    "            image_id=vid,\n",
    "        )\n",
    "        items.append(item)\n",
    "\n",
    "    with open(os.path.join(save_path, f'{mode}.json'), 'w') as wf:\n",
    "        json.dump(items, wf)\n",
    "    \n",
    "    gt = {\n",
    "        'annotations': [],\n",
    "        'images': [{'id': vid} for vid in splits_zh[mode]],\n",
    "    }\n",
    "\n",
    "    for vid in splits_zh[mode]:\n",
    "        for caption in vid2captions_zh[vid]:\n",
    "            item = dict(\n",
    "                image_id=vid,\n",
    "                caption=caption,\n",
    "                id=caption_id,\n",
    "            )\n",
    "            caption_id += 1\n",
    "            gt['annotations'].append(item)\n",
    "    \n",
    "    with open(os.path.join(save_path, f'{mode}_gt.json'), 'w') as wf:\n",
    "        json.dump(gt, wf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Subsets for Semi-Supervised Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "SAVE_PATH = './annotations'\n",
    "\n",
    "def run(dataset='coco', lang='en', ratios=[0.01, 0.1, 1, 10], seeds=[0, 1, 2]):\n",
    "    root = os.path.join(SAVE_PATH, dataset, lang)\n",
    "    train_file = os.path.join(root, 'train.json')\n",
    "    assert os.path.exists(train_file), train_file\n",
    "\n",
    "    print('### load json path from', train_file)\n",
    "    data = json.load(open(train_file, 'r'))\n",
    "\n",
    "    id2item = defaultdict(list)\n",
    "    for item in data:\n",
    "        id2item[item['image_id']].append(item)\n",
    "    \n",
    "    ids = sorted(list(id2item.keys()))\n",
    "\n",
    "    print(f'### there are {len(id2item)} unique images/videos, {len(data)} vision-caption pairs')\n",
    "\n",
    "    save_path = os.path.join(root, 'subsets')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    for ratio in ratios:\n",
    "        n_unique_images = int(len(id2item) * ratio / 100)\n",
    "        if n_unique_images < 1:\n",
    "            print(f'{ratio} is not applicible')\n",
    "            continue\n",
    "\n",
    "        print(f'--- generating a training subset of {ratio}% ({n_unique_images}) unique images/videos')\n",
    "\n",
    "        for seed in seeds:\n",
    "            json_path = os.path.join(save_path, f'{ratio}%_{seed}.json')\n",
    "            if os.path.exists(json_path):\n",
    "                print(json_path, 'exists')\n",
    "                continue\n",
    "            \n",
    "            random.seed(seed)\n",
    "            \n",
    "            this_ids = random.sample(ids, n_unique_images)\n",
    "            this_data = []\n",
    "            for this_id in this_ids:\n",
    "                this_data.extend(id2item[this_id])\n",
    "\n",
    "            print(json_path)\n",
    "            with open(json_path, 'w') as wf:\n",
    "                json.dump(this_data, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### load json path from ./annotations/coco/en/train.json\n",
      "### there are 113287 unique images/videos, 566747 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (11) unique images/videos\n",
      "./annotations/coco/en/subsets/0.01%_0.json exists\n",
      "./annotations/coco/en/subsets/0.01%_1.json exists\n",
      "./annotations/coco/en/subsets/0.01%_2.json exists\n",
      "--- generating a training subset of 0.1% (113) unique images/videos\n",
      "./annotations/coco/en/subsets/0.1%_0.json exists\n",
      "./annotations/coco/en/subsets/0.1%_1.json exists\n",
      "./annotations/coco/en/subsets/0.1%_2.json exists\n",
      "--- generating a training subset of 1% (1132) unique images/videos\n",
      "./annotations/coco/en/subsets/1%_0.json exists\n",
      "./annotations/coco/en/subsets/1%_1.json exists\n",
      "./annotations/coco/en/subsets/1%_2.json exists\n",
      "--- generating a training subset of 10% (11328) unique images/videos\n",
      "./annotations/coco/en/subsets/10%_0.json exists\n",
      "./annotations/coco/en/subsets/10%_1.json exists\n",
      "./annotations/coco/en/subsets/10%_2.json exists\n",
      "### load json path from ./annotations/coco/ja/train.json\n",
      "### there are 113287 unique images/videos, 566435 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (11) unique images/videos\n",
      "./annotations/coco/ja/subsets/0.01%_0.json\n",
      "./annotations/coco/ja/subsets/0.01%_1.json\n",
      "./annotations/coco/ja/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (113) unique images/videos\n",
      "./annotations/coco/ja/subsets/0.1%_0.json\n",
      "./annotations/coco/ja/subsets/0.1%_1.json\n",
      "./annotations/coco/ja/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (1132) unique images/videos\n",
      "./annotations/coco/ja/subsets/1%_0.json\n",
      "./annotations/coco/ja/subsets/1%_1.json\n",
      "./annotations/coco/ja/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (11328) unique images/videos\n",
      "./annotations/coco/ja/subsets/10%_0.json\n",
      "./annotations/coco/ja/subsets/10%_1.json\n",
      "./annotations/coco/ja/subsets/10%_2.json\n"
     ]
    }
   ],
   "source": [
    "run('coco', 'en')\n",
    "run('coco', 'ja') # not used at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### load json path from ./annotations/flickr30k/en/train.json\n",
      "### there are 29000 unique images/videos, 145000 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/flickr30k/en/subsets/0.01%_0.json\n",
      "./annotations/flickr30k/en/subsets/0.01%_1.json\n",
      "./annotations/flickr30k/en/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (29) unique images/videos\n",
      "./annotations/flickr30k/en/subsets/0.1%_0.json\n",
      "./annotations/flickr30k/en/subsets/0.1%_1.json\n",
      "./annotations/flickr30k/en/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (290) unique images/videos\n",
      "./annotations/flickr30k/en/subsets/1%_0.json\n",
      "./annotations/flickr30k/en/subsets/1%_1.json\n",
      "./annotations/flickr30k/en/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2900) unique images/videos\n",
      "./annotations/flickr30k/en/subsets/10%_0.json\n",
      "./annotations/flickr30k/en/subsets/10%_1.json\n",
      "./annotations/flickr30k/en/subsets/10%_2.json\n",
      "### load json path from ./annotations/flickr30k/de/train.json\n",
      "### there are 29000 unique images/videos, 145000 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/flickr30k/de/subsets/0.01%_0.json exists\n",
      "./annotations/flickr30k/de/subsets/0.01%_1.json exists\n",
      "./annotations/flickr30k/de/subsets/0.01%_2.json exists\n",
      "--- generating a training subset of 0.1% (29) unique images/videos\n",
      "./annotations/flickr30k/de/subsets/0.1%_0.json exists\n",
      "./annotations/flickr30k/de/subsets/0.1%_1.json exists\n",
      "./annotations/flickr30k/de/subsets/0.1%_2.json exists\n",
      "--- generating a training subset of 1% (290) unique images/videos\n",
      "./annotations/flickr30k/de/subsets/1%_0.json exists\n",
      "./annotations/flickr30k/de/subsets/1%_1.json exists\n",
      "./annotations/flickr30k/de/subsets/1%_2.json exists\n",
      "--- generating a training subset of 10% (2900) unique images/videos\n",
      "./annotations/flickr30k/de/subsets/10%_0.json exists\n",
      "./annotations/flickr30k/de/subsets/10%_1.json exists\n",
      "./annotations/flickr30k/de/subsets/10%_2.json exists\n",
      "### load json path from ./annotations/flickr30k/fr/train.json\n",
      "### there are 29000 unique images/videos, 29000 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/flickr30k/fr/subsets/0.01%_0.json\n",
      "./annotations/flickr30k/fr/subsets/0.01%_1.json\n",
      "./annotations/flickr30k/fr/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (29) unique images/videos\n",
      "./annotations/flickr30k/fr/subsets/0.1%_0.json\n",
      "./annotations/flickr30k/fr/subsets/0.1%_1.json\n",
      "./annotations/flickr30k/fr/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (290) unique images/videos\n",
      "./annotations/flickr30k/fr/subsets/1%_0.json\n",
      "./annotations/flickr30k/fr/subsets/1%_1.json\n",
      "./annotations/flickr30k/fr/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2900) unique images/videos\n",
      "./annotations/flickr30k/fr/subsets/10%_0.json\n",
      "./annotations/flickr30k/fr/subsets/10%_1.json\n",
      "./annotations/flickr30k/fr/subsets/10%_2.json\n",
      "### load json path from ./annotations/flickr30k/cs/train.json\n",
      "### there are 29000 unique images/videos, 29000 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/flickr30k/cs/subsets/0.01%_0.json\n",
      "./annotations/flickr30k/cs/subsets/0.01%_1.json\n",
      "./annotations/flickr30k/cs/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (29) unique images/videos\n",
      "./annotations/flickr30k/cs/subsets/0.1%_0.json\n",
      "./annotations/flickr30k/cs/subsets/0.1%_1.json\n",
      "./annotations/flickr30k/cs/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (290) unique images/videos\n",
      "./annotations/flickr30k/cs/subsets/1%_0.json\n",
      "./annotations/flickr30k/cs/subsets/1%_1.json\n",
      "./annotations/flickr30k/cs/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2900) unique images/videos\n",
      "./annotations/flickr30k/cs/subsets/10%_0.json\n",
      "./annotations/flickr30k/cs/subsets/10%_1.json\n",
      "./annotations/flickr30k/cs/subsets/10%_2.json\n"
     ]
    }
   ],
   "source": [
    "run('flickr30k', 'en')  # not used at all\n",
    "run('flickr30k', 'de')\n",
    "run('flickr30k', 'fr')\n",
    "run('flickr30k', 'cs')  # not used at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### load json path from ./annotations/msrvtt/en/train.json\n",
      "### there are 6513 unique images/videos, 130260 vision-caption pairs\n",
      "0.01 is not applicible\n",
      "--- generating a training subset of 0.1% (6) unique images/videos\n",
      "./annotations/msrvtt/en/subsets/0.1%_0.json\n",
      "./annotations/msrvtt/en/subsets/0.1%_1.json\n",
      "./annotations/msrvtt/en/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (65) unique images/videos\n",
      "./annotations/msrvtt/en/subsets/1%_0.json\n",
      "./annotations/msrvtt/en/subsets/1%_1.json\n",
      "./annotations/msrvtt/en/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (651) unique images/videos\n",
      "./annotations/msrvtt/en/subsets/10%_0.json\n",
      "./annotations/msrvtt/en/subsets/10%_1.json\n",
      "./annotations/msrvtt/en/subsets/10%_2.json\n"
     ]
    }
   ],
   "source": [
    "run('msrvtt', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### load json path from ./annotations/vatex/en/train.json\n",
      "### there are 25006 unique images/videos, 250060 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/vatex/en/subsets/0.01%_0.json\n",
      "./annotations/vatex/en/subsets/0.01%_1.json\n",
      "./annotations/vatex/en/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (25) unique images/videos\n",
      "./annotations/vatex/en/subsets/0.1%_0.json\n",
      "./annotations/vatex/en/subsets/0.1%_1.json\n",
      "./annotations/vatex/en/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (250) unique images/videos\n",
      "./annotations/vatex/en/subsets/1%_0.json\n",
      "./annotations/vatex/en/subsets/1%_1.json\n",
      "./annotations/vatex/en/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2500) unique images/videos\n",
      "./annotations/vatex/en/subsets/10%_0.json\n",
      "./annotations/vatex/en/subsets/10%_1.json\n",
      "./annotations/vatex/en/subsets/10%_2.json\n",
      "### load json path from ./annotations/vatex/zh/train.json\n",
      "### there are 25006 unique images/videos, 250060 vision-caption pairs\n",
      "--- generating a training subset of 0.01% (2) unique images/videos\n",
      "./annotations/vatex/zh/subsets/0.01%_0.json\n",
      "./annotations/vatex/zh/subsets/0.01%_1.json\n",
      "./annotations/vatex/zh/subsets/0.01%_2.json\n",
      "--- generating a training subset of 0.1% (25) unique images/videos\n",
      "./annotations/vatex/zh/subsets/0.1%_0.json\n",
      "./annotations/vatex/zh/subsets/0.1%_1.json\n",
      "./annotations/vatex/zh/subsets/0.1%_2.json\n",
      "--- generating a training subset of 1% (250) unique images/videos\n",
      "./annotations/vatex/zh/subsets/1%_0.json\n",
      "./annotations/vatex/zh/subsets/1%_1.json\n",
      "./annotations/vatex/zh/subsets/1%_2.json\n",
      "--- generating a training subset of 10% (2500) unique images/videos\n",
      "./annotations/vatex/zh/subsets/10%_0.json\n",
      "./annotations/vatex/zh/subsets/10%_1.json\n",
      "./annotations/vatex/zh/subsets/10%_2.json\n"
     ]
    }
   ],
   "source": [
    "run('vatex', 'en')  # not used at all\n",
    "run('vatex', 'zh')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How we obtain training corpora in 4 languages (`en`, `zh`, `de`, `fr`)\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{sharma2018conceptual,\n",
    "  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},\n",
    "  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},\n",
    "  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n",
    "  pages={2556--2565},\n",
    "  year={2018}\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. download the raw annotation files of Google's Conceptual Captions (GCC)\n",
    "2. concat the training and validation files, yielding 3,334,173 English captions\n",
    "3. pre-process captions by removing punctuations etc\n",
    "4. equally divide captions into three parts (1,111,391 English captions each) from front to back, corresponding to English captions parallel to Chinese, German, and French, respectively\n",
    "5. get the translated results of the 4th step's outputs via Google Translator (https://translate.google.com)\n",
    "6. randomly sample 1,111,391 English captions from the 2nd step's outputs\n",
    "\n",
    "Finally, we get 1,111,391 English-Chinese pairs, 1,111,391 English-German pairs, 1,111,391 English-French pairs, and 1,111,391 English-only sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-13 00:08:18--  https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv?_ga=2.191230122.-1896153081.1529438250\n",
      "正在解析主机 storage.googleapis.com (storage.googleapis.com)... 142.251.42.240, 172.217.163.48, 172.217.160.112, ...\n",
      "正在连接 storage.googleapis.com (storage.googleapis.com)|142.251.42.240|:443... 已连接。\n",
      "警告: 无法验证 storage.googleapis.com 的由 “CN=GTS CA 1C3,O=Google Trust Services LLC,C=US” 颁发的证书:\n",
      "  无法本地校验颁发者的权限。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：564607502 (538M) [application/octet-stream]\n",
      "正在保存至: “GCC-training.tsv”\n",
      "\n",
      "GCC-training.tsv    100%[===================>] 538.45M  6.44MB/s  用时 1m 42s    \n",
      "\n",
      "2023-04-13 00:10:02 (5.26 MB/s) - 已保存 “GCC-training.tsv” [564607502/564607502])\n",
      "\n",
      "--2023-04-13 00:10:03--  https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv?_ga=2.141047602.-1896153081.1529438250\n",
      "正在解析主机 storage.googleapis.com (storage.googleapis.com)... 142.251.43.16, 172.217.160.80, 172.217.163.48, ...\n",
      "正在连接 storage.googleapis.com (storage.googleapis.com)|142.251.43.16|:443... 已连接。\n",
      "警告: 无法验证 storage.googleapis.com 的由 “CN=GTS CA 1C3,O=Google Trust Services LLC,C=US” 颁发的证书:\n",
      "  无法本地校验颁发者的权限。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：2603670 (2.5M) [text/tab-separated-values]\n",
      "正在保存至: “GCC-1.1.0-Validation.tsv”\n",
      "\n",
      "GCC-1.1.0-Validatio 100%[===================>]   2.48M  2.97MB/s  用时 0.8s      \n",
      "\n",
      "2023-04-13 00:10:04 (2.97 MB/s) - 已保存 “GCC-1.1.0-Validation.tsv” [2603670/2603670])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O GCC-training.tsv https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv?_ga=2.191230122.-1896153081.1529438250 --no-check-certificate\n",
    "!wget -O GCC-1.1.0-Validation.tsv https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv?_ga=2.141047602.-1896153081.1529438250 --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drew inspiration from the pre-processing of ALBEF (https://github.com/salesforce/ALBEF)\n",
    "def clean(caption):\n",
    "    # this function is induced step by step via assertation\n",
    "    # success for SBU\n",
    "    def clean_token(token):\n",
    "        if '-' in token:\n",
    "            return\n",
    "        out = [char for char in token if char not in '\\',.();!!:#\\\"*~?']\n",
    "        if len(out):\n",
    "            return ''.join(out)\n",
    "        return False\n",
    "\n",
    "    def _split(token, sep):\n",
    "        if not isinstance(token, list):\n",
    "            token = [token]\n",
    "\n",
    "        out = []\n",
    "        for t in token:\n",
    "            out.extend(t.split(sep))\n",
    "        return out\n",
    "\n",
    "    def split(token, sep='-/'):\n",
    "        for char in sep:\n",
    "            token = _split(token, char)\n",
    "        return token\n",
    "\n",
    "    tokens = []\n",
    "    for token in caption.lower().strip().split(' '):\n",
    "        # if token in ['&', '@', '+', '=', '|', '_', '>', '<', '{', '}', '$'] or token not in string.punctuation:\n",
    "        tokens.extend(split(token))\n",
    "\n",
    "    tokens = [clean_token(token) for token in tokens if clean_token(token)]\n",
    "\n",
    "    return ' '.join(tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "\n",
    "with open('GCC-training.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        caption, url = line.strip().split('\\t')\n",
    "        captions.append(clean(caption))\n",
    "\n",
    "with open('GCC-1.1.0-Validation.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        caption, url = line.strip().split('\\t')\n",
    "        captions.append(clean(caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3334173"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zh 1111391 cc3m_en_to_zh.txt\n",
      "de 1111391 cc3m_en_to_de.txt\n",
      "fr 1111391 cc3m_en_to_fr.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "langs = ['zh', 'de', 'fr']\n",
    "\n",
    "captions_splits = np.array_split(captions, len(langs))\n",
    "for lang, data in zip(langs, captions_splits):\n",
    "    fn = f'cc3m_en_to_{lang}.txt'\n",
    "    with open(os.path.join(fn), 'w') as f:\n",
    "        f.write('\\n'.join(data.tolist()))\n",
    "    \n",
    "    print(lang, len(data), fn)\n",
    "\n",
    "# now, you should translate these generated files to the specific target language by yourself\n",
    "# we use Google Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "with open(os.path.join('cc3m_en.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(random.sample(captions, len(captions_splits[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e009a682218d944d4ca170df67ae304433d3da7f548affd1bafd64b5b995fea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
